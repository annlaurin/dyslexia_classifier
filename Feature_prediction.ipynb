{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444f024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 09:25:06.660551: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-16 09:25:06.732413: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-16 09:25:06.750223: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "#from IPython.display import set_matplotlib_formats\n",
    "#matplotlib_inline.backend_inline.set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "# ## tqdm for loading bars\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# for one-hot encoding\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from mlm_pytorch import MLM\n",
    "\n",
    "# Transformer wrapper\n",
    "import tensorflow as tf\n",
    "from x_transformers import TransformerWrapper, Encoder\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# Positional encoding in two dimensions\n",
    "from positional_encodings.torch_encodings import PositionalEncodingPermute1D, PositionalEncoding1D\n",
    "\n",
    "from functools import reduce\n",
    "import math\n",
    "\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"saved_models/simple_transformer\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66f8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.5\n",
    "    import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21cf40",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91aec7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "NUM_FEATURES = 7\n",
    "NUM_FIX = 30 \n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 31\n",
    "\n",
    "\n",
    "num_fix = NUM_FIX\n",
    "seq_len = NUM_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8823925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "    \n",
    "# def mse_loss(target, input, ignored_index):\n",
    "#     mask = target == ignored_index\n",
    "#     out = (input[~mask]-target[~mask])**2\n",
    "#     return out.mean()\n",
    "\n",
    "def mse_loss(target, input, mask):\n",
    "    out = (input[mask]-target[mask])**2\n",
    "    return out.mean()\n",
    "\n",
    "def mask_with_tokens_3D(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    reduced = torch.any(mask, dim=-1, keepdim=True)\n",
    "    expanded = reduced.expand_as(mask)\n",
    "    return expanded\n",
    "\n",
    "def get_mask_subset_with_prob_3D(mask, prob):\n",
    "    batch, num_fix, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * num_fix)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-2, keepdim=True)\n",
    "    mask_excess = (mask.cumsum(dim=-2)[:,:,0] > (num_tokens[:,:,0] * prob).ceil())\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, num_fix, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-2)\n",
    "    sampled_indices = (sampled_indices[:,:,0] + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, num_fix + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    new_mask = new_mask[:, 1:].bool()\n",
    "    \n",
    "    return new_mask.unsqueeze_(2).expand(-1,-1, seq_len)\n",
    "    \n",
    "\n",
    "def prob_mask_like_3D(t, prob):\n",
    "    temp = torch.zeros_like(t[:,:,0]).float().uniform_(0, 1) < prob\n",
    "    return temp.unsqueeze_(2).expand(-1,-1, seq_len)\n",
    "\n",
    "\n",
    "# class CustomPositionalEncoding(nn.Module):\n",
    "#   \"\"\"Learnable positional encoding for features \"\"\"\n",
    "#     def __init__(self, embed_dim, max_len=5000):\n",
    "#         super(CustomPositionalEncoding, self).__init__()\n",
    "        \n",
    "#         # Initialize a learnable positional encoding matrix\n",
    "#         self.encoding = nn.Parameter(torch.zeros(max_len, embed_dim))\n",
    "#         nn.init.xavier_uniform_(self.encoding)  # Xavier initialization for better training stability\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Add the learnable positional encoding to the input tensor\n",
    "#         return x + self.encoding[:x.size(1), :]\n",
    "\n",
    "    \n",
    "class CustomPositionalEncoding(nn.Module):\n",
    "    \"\"\"Learnable positional encoding for both features and fixations\"\"\"\n",
    "    def __init__(self, fixations, embed_dim, max_len=5000):\n",
    "        super(CustomPositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Initialize a learnable positional encoding matrix\n",
    "        self.encoding = nn.Parameter(torch.zeros(fixations, embed_dim))\n",
    "        nn.init.xavier_uniform_(self.encoding)  # Xavier initialization for better training stability\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        if mask is not None:\n",
    "            # Apply the mask to ignore padded positions\n",
    "            pos_encoding = self.encoding * mask\n",
    "        # Assumes input `x` is of shape [batch_size, fixations, embed_dim]\n",
    "        return x + pos_encoding#.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    \n",
    "def pad_group_with_zeros(group, target_rows):\n",
    "    # Calculate the number of rows to add\n",
    "    num_missing_rows = target_rows - len(group)\n",
    "    if num_missing_rows > 0:\n",
    "        # Create a DataFrame with the required number of padding rows\n",
    "        # input padding\n",
    "        zero_rows = pd.DataFrame(0.3333, index=range(num_missing_rows), columns=group.columns)\n",
    "        # Label padding\n",
    "        # zero_rows.iloc[:, 0] = 31\n",
    "        # Concatenate the group with the zero rows\n",
    "        group = pd.concat([group, zero_rows], ignore_index=True)\n",
    "    return group\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert Series in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        trial, label = sample['trial'], sample['label']\n",
    "        trial = torch.from_numpy(trial).float()\n",
    "        label = torch.from_numpy(label).float()\n",
    "        return trial, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e747c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSCFixationsOrder(Dataset):\n",
    "    \"\"\"Dataset with the long-format sequence of fixations made during reading by dyslexic \n",
    "    and normally-developing Russian-speaking monolingual children.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None, target_transform = None, \n",
    "                 n_fix = NUM_FIX, \n",
    "                 dropPhonologyFeatures = True, dropPhonologySubjects = True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied\n",
    "                on a label.\n",
    "        \"\"\"\n",
    "        self.fixations_frame = pd.read_csv(csv_file)\n",
    "        \n",
    "        # remove demography and identification\n",
    "        self.fixations_frame = self.fixations_frame.drop(columns = ['fix_x', \n",
    "                                                                    'fix_y', 'full_text' \n",
    "                                                                    ]) \n",
    "        \n",
    "        #self.fixations_frame = self.fixations_frame[order_cols]\n",
    "        \n",
    "        # Log-transforming appropriate measures\n",
    "        to_transform = ['frequency', 'fix_dur'] \n",
    "        for column in to_transform:\n",
    "            self.fixations_frame[column] = self.fixations_frame[column].apply(lambda x: np.log(x) if x > 0 else 0) \n",
    "\n",
    "        # Center \n",
    "        cols = ['fix_dur', 'landing', 'predictability',\n",
    "                'frequency', 'word_length', 'number.morphemes', \n",
    "                'next_fix_dist', 'sac_ampl', 'sac_angle', \n",
    "                'sac_vel']\n",
    "        for col in cols:\n",
    "            self.fixations_frame[col] = np.where(self.fixations_frame[col] == 0, -4,\n",
    "                (self.fixations_frame[col] - self.fixations_frame[col].mean())/self.fixations_frame[col].std(ddof=0)) \n",
    "        \n",
    "\n",
    "        # Drop padding \n",
    "        self.fixations_frame = self.fixations_frame[self.fixations_frame['fix_dur'] != -4]\n",
    "        \n",
    "        \n",
    "        # Convert direction to a dummy-coded variable\n",
    "        self.fixations_frame['direction'] = np.where(self.fixations_frame['direction'].isnull(), 0,\n",
    "                                                     self.fixations_frame['direction'])\n",
    "        self.fixations_frame = pd.concat([self.fixations_frame, \n",
    "                                          pd.get_dummies(self.fixations_frame['direction'], \n",
    "                                                         prefix='dummy')], axis=1)\n",
    "        \n",
    "        # Center exceptions: \n",
    "        # columns where 0 is meaningful and should not be counted as NA\n",
    "#         cols = ['rel.position', 'dummy_DOWN', 'dummy_LEFT','dummy_RIGHT', 'dummy_UP']\n",
    "#         for col in cols:\n",
    "#             self.fixations_frame[col] = (self.fixations_frame[col] - \\\n",
    "#                                              self.fixations_frame[col].mean())/self.fixations_frame[col].std(ddof=0)\n",
    "            \n",
    "        if dropPhonologySubjects == True:\n",
    "            # Drop subjects\n",
    "            self.fixations_frame.dropna(axis = 0, how = 'any', inplace = True)\n",
    "        else:\n",
    "            # Drop columns\n",
    "            self.fixations_frame.dropna(axis = 1, how = 'any', inplace = True)\n",
    "        \n",
    "        \n",
    "        self.fixations_frame['subj'] = self.fixations_frame['subj'].astype(str)\n",
    "        self.fixations_frame['item'] = self.fixations_frame['sn'].astype(str)\n",
    "        self.fixations_frame['Combined'] = self.fixations_frame[['subj', 'item']].agg('_'.join, axis=1)\n",
    "        \n",
    "        # cleaning up\n",
    "        self.fixations_frame.drop(columns = ['subj', 'item', 'direction', 'dummy_0', 'sn',\\\n",
    "                                            'dummy_DOWN', 'dummy_LEFT', 'dummy_UP'\\\n",
    "                                            ], inplace = True)\n",
    "        self.fixations_frame.drop(columns = ['word_length', 'predictability', \n",
    "                                             'frequency', 'number.morphemes', 'fix_index'], \n",
    "                                             inplace = True)\n",
    "        \n",
    "\n",
    "        padded = self.fixations_frame.groupby('Combined', group_keys=False).apply(lambda x: \n",
    "                                                                           pad_group_with_zeros(x, n_fix))\n",
    "        padded.drop(columns = \"Combined\", inplace = True)\n",
    "    \n",
    "        \n",
    "        #### Fixation index is the label\n",
    "        self.fixations_frame = padded.to_numpy()\n",
    "        dataReshaped = np.reshape(self.fixations_frame, (int(len(self.fixations_frame)/n_fix), \n",
    "                                                         n_fix, self.fixations_frame.shape[1]))\n",
    "\n",
    "        self.predictors = dataReshaped[:,:,1:]\n",
    "        self.labels = dataReshaped[:,:,0]\n",
    "        self.labels = to_categorical(self.labels-1, num_classes=NUM_CLASSES)   # one-hot encoding\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        trial = self.predictors[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        sample = {'trial': trial, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            sample = self.target_transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8466dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = RSCFixationsOrder(csv_file='data/RSC_long_no_word_padded_word_pos.csv', \n",
    "                                    transform=ToTensor(),\n",
    "                                    dropPhonologySubjects = True)\n",
    "\n",
    "train_size = int(0.8 * len(transformed_dataset))\n",
    "val_size = int(0.1 * len(transformed_dataset))\n",
    "test_size = len(transformed_dataset) - val_size - train_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(transformed_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511ba45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 30, 7])\n",
      "Labels batch shape: torch.Size([64, 30, 31])\n"
     ]
    }
   ],
   "source": [
    "trials, labels = next(iter(train_loader))\n",
    "\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa35291a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6186e+00,  1.3636e-03,  4.8822e-02,  2.0223e-01, -4.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 2.1263e-01,  5.3885e-01,  6.6894e-01, -2.4092e-01, -4.0000e+00,\n",
       "          1.0000e-01,  1.0000e+00],\n",
       "        [ 4.5904e-01,  3.6327e-01,  5.0904e-01,  4.1091e+00, -4.0000e+00,\n",
       "          2.0000e-01,  0.0000e+00],\n",
       "        [ 9.0372e-01,  1.0978e+00,  1.2774e+00, -1.1564e-01, -4.0000e+00,\n",
       "          1.0000e-01,  1.0000e+00],\n",
       "        [ 1.4327e+00,  1.0728e+00,  2.0483e-01, -6.6295e-02, -4.0000e+00,\n",
       "          3.0000e-01,  1.0000e+00],\n",
       "        [ 6.1280e-01, -5.2385e-02, -1.8799e-03, -7.7553e-02, -4.0000e+00,\n",
       "          5.0000e-01,  1.0000e+00],\n",
       "        [ 1.8812e+00,  2.1636e-01,  1.7753e-01, -5.4318e-02, -4.0000e+00,\n",
       "          5.0000e-01,  1.0000e+00],\n",
       "        [ 6.6885e-01,  2.8802e-01,  2.6723e-01, -1.5469e-01, -4.0000e+00,\n",
       "          6.0000e-01,  1.0000e+00],\n",
       "        [ 3.0441e+00,  4.0985e-01,  3.0623e-01,  1.1316e-02, -4.0000e+00,\n",
       "          7.0000e-01,  1.0000e+00],\n",
       "        [-1.7804e-01,  2.6294e-01,  3.1403e-01,  4.0957e+00, -4.0000e+00,\n",
       "          9.0000e-01,  0.0000e+00],\n",
       "        [-2.9922e-01,  4.0003e+00,  4.1830e+00, -6.9701e-01, -4.0000e+00,\n",
       "          8.0000e-01,  1.0000e+00],\n",
       "        [ 3.3330e-01,  3.3330e-01,  3.3330e-01,  3.3330e-01,  3.3330e-01,\n",
       "          3.3330e-01,  3.3330e-01],\n",
       "        [ 3.3330e-01,  3.3330e-01,  3.3330e-01,  3.3330e-01,  3.3330e-01,\n",
       "          3.3330e-01,  3.3330e-01],\n",
       "        [ 3.3330e-01,  3.3330e-01,  3.3330e-01,  3.3330e-01,  3.3330e-01,\n",
       "          3.3330e-01,  3.3330e-01],\n",
       "        [ 3.3330e-01,  3.3330e-01,  3.3330e-01,  3.3330e-01,  3.3330e-01,\n",
       "          3.3330e-01,  3.3330e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials[1, 0:15,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508254d",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7d60f",
   "metadata": {},
   "source": [
    "-- Instead of reconstructing the vocabulary, we would use MSE and try to reconstruct *x, y, fix dur* and we can look into how to reconstruct the rest of the features (i.e. all the additional that you used).\n",
    "\n",
    "-- and an adjustable pytorch implementation here:\n",
    "https://github.com/lucidrains/mlm-pytorch/tree/master\n",
    "\n",
    "-- https://github.com/lucidrains/mlm-pytorch\n",
    "\n",
    "-- Embedding for position - DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "186d3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len, l2norm_embed = False):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5 if not l2norm_embed else 1.\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "    def forward(self, x, pos = None, seq_start_pos = None):\n",
    "        seq_len, device = x.shape[1], x.device\n",
    "        assert seq_len <= self.max_seq_len, f'you are passing in a sequence length of {seq_len} but your absolute positional embedding has a max sequence length of {self.max_seq_len}'\n",
    "\n",
    "#         if not exists(pos):\n",
    "#             pos = torch.arange(seq_len, device = device)\n",
    "\n",
    "#         if exists(seq_start_pos):\n",
    "#             pos = (pos - seq_start_pos[..., None]).clamp(min = 0)\n",
    "\n",
    "        pos_emb = self.emb(pos)\n",
    "        pos_emb = pos_emb * self.scale\n",
    "        return l2norm(pos_emb) if self.l2norm_embed else pos_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b50527c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2423601/1419359578.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pos_emb' is not defined"
     ]
    }
   ],
   "source": [
    "# pos = torch.arange(seq_len, device = device)\n",
    "# pos = pos_emb(pos)\n",
    "# pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162735ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_emb = AbsolutePositionalEmbedding(dim=seq_len, max_seq_len = num_fix)\n",
    "# pos_emb(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Embedding(30, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7045bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annas changes\n",
    "class TransformerWithCustomPositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim, \n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        max_len=5000,\n",
    "        mask_prob = 0.15,\n",
    "        replace_prob = 1, # 0.9\n",
    "        mask_token_id = 2,\n",
    "        pad_token_id = 0.3333,\n",
    "        mask_ignore_token_ids = []\n",
    "        ):\n",
    "        super(TransformerWithCustomPositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "        \n",
    "        # positional encoding\n",
    "        self.positional_encoding = CustomPositionalEncoding(num_fix, embed_dim).to(device)\n",
    "        \n",
    "        # transformer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                        nhead=num_heads, \n",
    "                                                        batch_first = True)\n",
    "        self.transformer = TransformerEncoder(self.encoder_layer, num_layers=num_layers).to(device)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens_3D(seq, self.mask_ignore_token_ids) \n",
    "        mask = get_mask_subset_with_prob_3D(~no_mask, self.mask_prob)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_seq = seq.clone().detach()\n",
    "        masked_seq_pos = self.positional_encoding(masked_seq, mask = ~no_mask)\n",
    "\n",
    "        # [mask] input\n",
    "        masked_replace_prob = prob_mask_like_3D(seq, self.replace_prob) # Anna: select 90% of all values  (ignore all masking for now)\n",
    "        masked_seq = masked_seq_pos.masked_fill(mask * masked_replace_prob, self.mask_token_id) # Anna: select 90% only of those selected for masking\n",
    "        \n",
    "        # derive labels to predict\n",
    "        labels = seq # self.positional_encoding(seq) # add positional encoding before masking, so that encoding does not affect mask\n",
    "        labels = labels.masked_fill(~mask, self.pad_token_id)\n",
    "        \n",
    "        # Pass through the transformer\n",
    "        preds = self.transformer(masked_seq)\n",
    "        \n",
    "        my_loss = mse_loss(\n",
    "            labels,\n",
    "            preds,\n",
    "            #ignored_index = self.pad_token_id\n",
    "            mask = mask\n",
    "        )\n",
    "\n",
    "        return preds, my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a63406ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = trials\n",
    "# seq = X_train\n",
    "\n",
    "# mask_prob = 0.15\n",
    "# replace_prob = 1\n",
    "# mask_token_id = 2\n",
    "# pad_token_id = 0.3333\n",
    "# mask_ignore_token_ids = set([pad_token_id])\n",
    "# embed_dim = 10\n",
    "\n",
    "# positional_encoding = CustomPositionalEncoding(num_fix, seq_len).to(device)\n",
    "\n",
    "# # transformer\n",
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "#                                                 nhead=2, \n",
    "#                                                 batch_first = True)\n",
    "# transformer = TransformerEncoder(encoder_layer, num_layers=2).to(device)\n",
    "\n",
    "# no_mask = mask_with_tokens_3D(seq, mask_ignore_token_ids) # works in 3D\n",
    "# mask = get_mask_subset_with_prob_3D(~no_mask, mask_prob)\n",
    "\n",
    "# # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "# masked_seq = seq.clone().detach()\n",
    "# masked_seq_pos = positional_encoding(masked_seq, mask = ~no_mask)\n",
    "\n",
    "# # [mask] input\n",
    "# masked_replace_prob = prob_mask_like_3D(seq, replace_prob) # Anna: select 90% of all values  (ignore all masking for now)\n",
    "# masked_seq = masked_seq_pos.masked_fill(mask * masked_replace_prob, mask_token_id) # Anna: select 90% only of those selected for masking\n",
    "\n",
    "# masked_seq[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e312fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # derive labels to predict\n",
    "# labels = seq # self.positional_encoding(seq) # add positional encoding before masking, so that encoding does not affect mask\n",
    "# labels = labels.masked_fill(~mask, pad_token_id)\n",
    "\n",
    "# labels[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels[0,1] == seq[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6a06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerWithCustomPositionalEncoding(\n",
      "  (positional_encoding): CustomPositionalEncoding()\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=7, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=7, bias=True)\n",
      "    (norm1): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=7, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=7, bias=True)\n",
      "        (norm1): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((7,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annlaurin/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Ok, let's see:\n",
    "trainer = TransformerWithCustomPositionalEncoding(num_heads = 1, \n",
    "                                                  num_layers= 2, \n",
    "                                                  embed_dim = seq_len).cuda()\n",
    "\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fb3021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.2353, Test Loss: 2.1076\n",
      "Epoch 11: Train Loss: 1.3506, Test Loss: 1.2883\n",
      "Epoch 21: Train Loss: 1.2263, Test Loss: 1.2021\n",
      "Epoch 31: Train Loss: 1.1994, Test Loss: 1.1825\n",
      "Epoch 41: Train Loss: 1.1741, Test Loss: 1.1848\n",
      "Epoch 51: Train Loss: 1.1644, Test Loss: 1.2053\n",
      "Epoch 61: Train Loss: 1.1734, Test Loss: 1.1306\n",
      "Epoch 71: Train Loss: 1.1592, Test Loss: 1.1860\n",
      "Epoch 81: Train Loss: 1.1578, Test Loss: 1.2287\n"
     ]
    }
   ],
   "source": [
    "# # Setup optimizer to optimize model's parameters\n",
    "optimizer = Adam(trainer.parameters(), lr=3e-4)\n",
    "\n",
    "epochs = 81\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    ### Training\n",
    "    trainer.train()\n",
    "    \n",
    "    for X_train, y_train in train_loader:\n",
    "        # 1. Forward pass \n",
    "        # 2. Calculate loss/accuracy\n",
    "        train_preds, loss = trainer(X_train)\n",
    "        \n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backwards\n",
    "        loss.backward()\n",
    " \n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * X_train.size(0)\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    #### Evaluation\n",
    "    test_loss = 0.0\n",
    "    trainer.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            test_preds, tloss = trainer(X_test)\n",
    "            test_loss += tloss.item() * X_test.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "336f8c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2460, -0.0846,  0.0449,  0.2025,  0.4812,  0.0000,  1.0000],\n",
       "        [-0.1541,  0.6463,  0.8288, -0.0110,  1.4887,  0.0909,  1.0000],\n",
       "        [ 0.4718,  0.2880,  0.4778, -0.0474,  1.0629,  0.2727,  1.0000],\n",
       "        [ 1.6479,  0.7001,  0.7508, -0.0155,  1.0056,  0.3636,  1.0000],\n",
       "        [ 0.2884,  0.5639,  0.7391,  0.0501,  0.9180,  0.5455,  1.0000],\n",
       "        [ 0.9838,  0.7431,  0.9654,  0.0322,  1.3550,  0.6364,  1.0000],\n",
       "        [ 3.1893,  0.2450,  0.4388,  0.0614,  0.9648,  0.7273,  1.0000],\n",
       "        [ 1.0871,  3.8139,  4.3000, -0.8036,  3.7348,  0.9091,  1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[6, 0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "724035a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0681,  0.5923,  0.4687,  0.0894,  1.0752,  0.4437,  0.8271],\n",
       "        [ 1.0517,  0.6961,  0.5399,  0.0664,  1.0765,  0.4452,  0.8137],\n",
       "        [ 1.0538,  0.3869,  0.3900,  0.0081,  0.9554,  0.4292,  0.8181],\n",
       "        [ 1.0685,  0.5520,  0.3248,  0.0790,  0.9086,  0.4410,  0.8213],\n",
       "        [ 1.0538,  0.3869,  0.3900,  0.0081,  0.9554,  0.4292,  0.8181],\n",
       "        [ 1.0357,  0.6291,  0.2132, -0.0023,  0.5807,  0.4299,  0.7967],\n",
       "        [ 1.0453,  0.7976,  0.3094,  0.0062,  0.5926,  0.4323,  0.8061],\n",
       "        [ 1.0034,  0.5951,  0.7288, -0.0642,  1.0518,  0.4237,  0.8004]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[6, 0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb774ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

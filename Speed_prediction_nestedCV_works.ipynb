{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfc86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import ceil\n",
    "import random\n",
    "import argparse\n",
    "import copy\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "## Import custom parts of the project\n",
    "#import data\n",
    "#import models\n",
    "#import roc\n",
    "#import constants as const\n",
    "#import notify\n",
    "\n",
    "from typing import TextIO, Callable, Collection, Dict, Iterator, List, Tuple, Type, TypeVar\n",
    "\n",
    "#from data import EyetrackingDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch import nn # nn contains all of PyTorch's building blocks for neural networks\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset \n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BATCH_SUBJECTS = True\n",
    "dropPhonologyFeatures = True\n",
    "T = TypeVar(\"T\", bound=\"EyetrackingClassifier\")\n",
    "\n",
    "ablation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c884760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c521ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinguisticFeature = Callable[[Tuple[str]], Tuple[torch.Tensor]]\n",
    "\n",
    "# def apply_standardization(x, m, sd):\n",
    "#     nonzero_sd = sd.clone()\n",
    "#     nonzero_sd[sd == 0] = 1\n",
    "#     x = torch.from_numpy(x).float()\n",
    "#     res = (x - m.unsqueeze(0)) / nonzero_sd.unsqueeze(0)\n",
    "#     return res\n",
    "\n",
    "def apply_standardization(x, m, sd):\n",
    "    nonzero_sd = sd.clone()\n",
    "    nonzero_sd[sd == 0] = 1\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x_zeros = x[x.sum(dim=(1)) == 0]\n",
    "    x_zeros[x_zeros==0] = -5\n",
    "    x_non_zeros = x[x.sum(dim=(1)) != 0]\n",
    "    x_non_zeros = (x_non_zeros - m.unsqueeze(0)) / nonzero_sd.unsqueeze(0)\n",
    "    res = torch.cat((x_non_zeros, x_zeros), axis =0)\n",
    "    return res\n",
    "\n",
    "\n",
    "def aggregate_per_subject(subjs, y_preds, y_preds_class, y_trues):\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_preds_class = np.array(y_preds_class)\n",
    "    y_trues = np.array(y_trues)\n",
    "    subjs = np.array(subjs).flatten()\n",
    "    y_preds_subj = []\n",
    "    y_preds_class_subj = []\n",
    "    y_trues_subj = []\n",
    "    subjs_subj = np.unique(subjs)\n",
    "    for subj in subjs_subj:\n",
    "        subj = subj.item()\n",
    "        y_pred_class_subj = y_preds_class[subjs == subj]\n",
    "        y_pred_subj = y_preds[subjs == subj]\n",
    "        y_true_subj = y_trues[subjs == subj]\n",
    "        assert len(np.unique(y_true_subj)) == 1, f\"No unique label: subj={subj}\"\n",
    "        y_trues_subj.append(np.unique(y_true_subj).item())\n",
    "        y_preds_subj.append(np.mean(y_pred_subj).item())\n",
    "        if sum(y_pred_class_subj) >= (len(y_pred_class_subj) / 2):\n",
    "            y_preds_class_subj.append(1)\n",
    "        else:\n",
    "            y_preds_class_subj.append(0)\n",
    "    return subjs_subj, y_preds_subj, y_preds_class_subj, y_trues_subj\n",
    "\n",
    "def getmeansd(dataset, batch: bool = False):  # removing rows of 0s\n",
    "    if batch:\n",
    "        # Anna added preprocessing from ndarray to torch\n",
    "        tensors = [X for X, _, _, _ in dataset]  #torch.from_numpy(X).float()\n",
    "        tensors = torch.cat(tensors, axis=0)\n",
    "        # remove padded tensors\n",
    "        tensors = tensors[tensors.sum(dim=(1,2)) != 0]   #tensors[tensors.sum(dim=(1, 2)) != 0]\n",
    "        # remove rows of 0s from the computation\n",
    "        sentences, timesteps, features = tensors.size()\n",
    "        subset = tensors.sum(dim=(2)) != 0\n",
    "        subset = subset.view(sentences, timesteps, 1)\n",
    "        subset = subset.expand(sentences, timesteps, features)\n",
    "        result = tensors[subset].view(-1, features) \n",
    "        \n",
    "        means = torch.mean(result, dim=(0))\n",
    "        sd = torch.std(result, dim=(0))\n",
    "        return means, sd\n",
    "    else:\n",
    "        tensors = [torch.from_numpy(X).float() for X, _, _, _ in dataset] # Anna added , was [X for X, _, _ in dataset]\n",
    "        tensors = torch.cat(tensors, axis=0)\n",
    "        # remove padded tensors\n",
    "        tensors = tensors[tensors.sum(dim=1) != 0]\n",
    "        means = torch.mean(tensors, 0)\n",
    "        sd = torch.std(tensors, 0)\n",
    "        return means, sd\n",
    "    \n",
    "    \n",
    "def get_params(paramdict) -> dict:\n",
    "    selected_pars = dict()\n",
    "    for k in paramdict:\n",
    "        selected_pars[k] = random.sample(list(paramdict[k]), 1)[0]\n",
    "    return selected_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d642915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_speed_per_subject(subjs, y_preds, y_trues):\n",
    "    y_preds = np.array(y_preds)\n",
    "    #y_preds_class = np.array(y_preds_class)\n",
    "    y_trues = np.array(y_trues)\n",
    "    subjs = np.array(subjs).flatten()\n",
    "    y_preds_subj = []\n",
    "    y_trues_subj = []\n",
    "    subjs_subj = np.unique(subjs)\n",
    "    for subj in subjs_subj:\n",
    "        subj = subj.item()\n",
    "        y_pred_subj = y_preds[subjs == subj]\n",
    "        y_true_subj = y_trues[subjs == subj]\n",
    "        assert len(np.unique(y_true_subj)) == 1, f\"No unique label: subj={subj}\"\n",
    "        y_trues_subj.append(np.unique(y_true_subj).item())\n",
    "        y_preds_subj.append(np.mean(y_pred_subj).item())\n",
    "\n",
    "    return subjs_subj, y_preds_subj, y_trues_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9009c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyetrackingDataPreprocessor(Dataset):\n",
    "    \"\"\"Dataset with the long-format sequence of fixations made during reading by dyslexic \n",
    "    and normally-developing Russian-speaking monolingual children.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        csv_file, \n",
    "        transform=None, \n",
    "        target_transform=None, \n",
    "        dropPhonologyFeatures = True, \n",
    "        dropPhonologySubjects = True,     \n",
    "        num_folds: float = 10,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied\n",
    "                on a label.\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # changing dyslexia labels to 0 and 1\n",
    "        data['group'] = data['group'] + 0.5\n",
    "        \n",
    "        # log-transforming frequency\n",
    "        to_transform = ['frequency', 'predictability', 'fix_dur'] #\n",
    "        for column in to_transform:\n",
    "            data[column] = data[column].apply(lambda x: np.log(x) if x > 0 else 0) \n",
    "        \n",
    "        # drop reading speed, the main basis of classification\n",
    "        data = data.drop(columns = ['fix_x', 'fix_y', 'fix_index'])  \n",
    "        \n",
    "        data['Reading_speed'] = (data['Reading_speed'] - data['Reading_speed'].mean())/data['Reading_speed'].std(ddof=0)\n",
    "        \n",
    "        if ablation == True:\n",
    "            data = data.drop(columns = ['sex', 'Grade'])\n",
    "            convert_columns = ['direction']\n",
    "        else:\n",
    "            convert_columns = ['Grade', 'direction'] \n",
    "        \n",
    "        if dropPhonologyFeatures == True:\n",
    "            data = data.drop(columns = ['IQ', 'Sound_detection', 'Sound_change'])\n",
    "        \n",
    "        for column in convert_columns:\n",
    "            prefix = column + '_dummy'\n",
    "            data = pd.concat([data, pd.get_dummies(data[column], \n",
    "                                    prefix=prefix)], axis=1)\n",
    "            data = data.drop(columns = column)\n",
    "        data = data.drop(columns = ['Grade_dummy_0', 'direction_dummy_0'])\n",
    "            \n",
    "        if dropPhonologySubjects == True:\n",
    "            # Drop subjects\n",
    "            data.dropna(axis = 0, how = 'any', inplace = True)\n",
    "        else:\n",
    "            # Drop columns\n",
    "            data.dropna(axis = 1, how = 'any', inplace = True)\n",
    "            \n",
    "        # rearrange columns (I need demogrpahic information to come last)\n",
    "        cols = ['item', 'subj', 'group', 'Reading_speed', 'fix_dur', 'landing', 'word_length',\n",
    "                 'predictability', 'frequency', 'number.morphemes', 'next_fix_dist',\n",
    "                 'sac_ampl', 'sac_angle', 'sac_vel', 'rel.position', 'direction_dummy_DOWN',\n",
    "                 'direction_dummy_LEFT', 'direction_dummy_RIGHT', 'direction_dummy_UP',\n",
    "                 'sex', 'Age', 'Grade_dummy_1', 'Grade_dummy_2', 'Grade_dummy_3', 'Grade_dummy_4',\n",
    "                 'Grade_dummy_5', 'Grade_dummy_6']\n",
    "        data = data[cols]\n",
    "        \n",
    "        # Record features that are used for prediction\n",
    "        self._features = [i for i in data.columns if i not in ['group', 'item', 'subj', 'Reading_speed']]\n",
    "        self._data = pd.DataFrame()\n",
    "        # Add sentence IDs and subject IDs\n",
    "        self._data[\"sn\"] = data[\"item\"]\n",
    "        self._data[\"subj\"] = data[\"subj\"]\n",
    "        # Add labels\n",
    "        self._data[\"group\"] = data[\"group\"]\n",
    "        self._data[\"reading_speed\"] = data[\"Reading_speed\"]\n",
    "        \n",
    "        # Add features used for prediction\n",
    "        for feature in self._features:\n",
    "            self._data[feature] = data[feature]\n",
    "\n",
    "        # Distribute subjects across stratified folds\n",
    "        self._num_folds = num_folds\n",
    "        self._folds = [[] for _ in range(num_folds)]\n",
    "        dyslexic_subjects = self._data[self._data[\"group\"] == 1][\"subj\"].unique()\n",
    "        control_subjects = self._data[self._data[\"group\"] == 0][\"subj\"].unique()\n",
    "        random.shuffle(dyslexic_subjects)\n",
    "        random.shuffle(control_subjects)\n",
    "        for i, subj in enumerate(dyslexic_subjects):\n",
    "            self._folds[i % num_folds].append(subj)\n",
    "        for i, subj in enumerate(control_subjects):\n",
    "            self._folds[num_folds - 1 - i % num_folds].append(subj)\n",
    "        for fold in self._folds:\n",
    "            random.shuffle(fold)\n",
    "\n",
    "    def _iter_trials(self, folds: Collection[int]) -> Iterator[pd.DataFrame]:\n",
    "        # Iterate over all folds\n",
    "        for fold in folds:\n",
    "            # Iterate over all subjects in the fold\n",
    "            for subj in self._folds[fold]:       # Anna: subj in fold?\n",
    "                subj_data = self._data[self._data[\"subj\"] == subj]\n",
    "                # Iterate over all sentences this subject read\n",
    "                for sn in subj_data[\"sn\"].unique():\n",
    "                    trial_data = subj_data[subj_data[\"sn\"] == sn]\n",
    "                    yield trial_data\n",
    "                    \n",
    "                    \n",
    "    def iter_folds(\n",
    "        self, folds: Collection[int]) -> Iterator[Tuple[torch.Tensor, torch.Tensor, int]]:\n",
    "        for trial_data in self._iter_trials(folds):\n",
    "            predictors = trial_data[self._features].to_numpy()\n",
    "            #predictors = np.reshape(predictors, (int(len(predictors)/278), 278, predictors.shape[1]))\n",
    "            label = trial_data[\"group\"].unique().item()\n",
    "            subj = trial_data[\"subj\"].unique().item()\n",
    "            reading_speed = trial_data[\"reading_speed\"].unique().item()\n",
    "            #  X = (time_steps, features)\n",
    "            X = predictors\n",
    "            y = torch.tensor(label, dtype=torch.float)\n",
    "            rs = torch.tensor(reading_speed , dtype=torch.float)\n",
    "            yield X, y, subj, rs\n",
    "                    \n",
    "\n",
    "    @property\n",
    "    def num_features(self) -> int:\n",
    "        \"\"\"Number of features per word (excluding word vector dimensions).\"\"\"\n",
    "        return len(self._features)\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def max_number_of_sentences(self):\n",
    "        data_copy = self._data.copy()\n",
    "        max_s_count = data_copy.groupby(by=\"subj\").sn.unique()\n",
    "        return max([len(x) for x in max_s_count])\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb621a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyetrackingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocessor: EyetrackingDataPreprocessor,\n",
    "       # word_vector_model: WordVectorModel,\n",
    "        folds: Collection[int],\n",
    "        batch_subjects: bool = False,\n",
    "    ):\n",
    "        self.sentences = list(preprocessor.iter_folds(folds))\n",
    "        self._subjects = list(np.unique([subj for _, _, subj, _ in self.sentences]))\n",
    "        self.num_features = preprocessor.num_features# + word_vector_model.dimensions()\n",
    "        self.batch_subjects = batch_subjects\n",
    "        #self.max_sentence_length = preprocessor.max_sentence_length\n",
    "        self.max_number_of_sentences = preprocessor.max_number_of_sentences\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        if self.batch_subjects:\n",
    "            subject = self._subjects[index]\n",
    "            subject_sentences = [\n",
    "                (X, y, subj, rs) for X, y, subj, rs in self.sentences if subj == subject\n",
    "            ]\n",
    "            X = torch.stack([torch.FloatTensor(X) for X, _, _, _ in subject_sentences]) #[X for X, _, _ in subject_sentences] #torch.FloatTensor([X for X, _, _ in subject_sentences])\n",
    "            y = torch.stack([y for _, y, _, _ in subject_sentences]).unique().squeeze() \n",
    "            rs = torch.stack([rs for _, _, _, rs in subject_sentences]).unique().squeeze()\n",
    "            return X, y, subject, rs\n",
    "\n",
    "        else:\n",
    "            X, y, subj, rs = self.sentences[index]\n",
    "            #X = torch.from_numpy(X).float()   \n",
    "            return X, y, subj, rs\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.batch_subjects:\n",
    "            return len(self._subjects)\n",
    "        else:\n",
    "            return len(self.sentences)\n",
    "\n",
    "    def standardize(self, mean: torch.Tensor, sd: torch.Tensor):\n",
    "        self.sentences = [\n",
    "            (apply_standardization(X, mean, sd), y, subj, rs)\n",
    "            for X, y, subj, rs in self.sentences\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "895611d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int, config):\n",
    "        super().__init__()\n",
    "        self.initialize_model(input_size, config)\n",
    "        self.config = config\n",
    "\n",
    "    def initialize_model(self, input_size: int, config):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _predict(self, X: torch.Tensor, subj_mean: bool = False, pretrain: bool = False) -> torch.Tensor:\n",
    "        if pretrain:\n",
    "            y_preds = self(X, pretrain=True)\n",
    "            # return predictions for all 12 features\n",
    "            return y_preds\n",
    "        if subj_mean:\n",
    "            # X = (batch_size, sentences, time_steps, features)\n",
    "            X_flat = X.view(X.size(0) * X.size(1), X.size(2), X.size(3))\n",
    "            # X_flat = (batch_size*sentences, time_steps, features)\n",
    "            y_pred_flat = self(X_flat, pretrain)\n",
    "            # y_pred_flat = (batch_size*sentences)\n",
    "            y_pred = y_pred_flat.view(X.size(0), X.size(1))\n",
    "            # y_pred = (batch_size, sentences)\n",
    "            final_predictions = []\n",
    "            for batch_X, batch_y_pred in zip(X, y_pred):\n",
    "                batch_predictions = []\n",
    "                for sentence_X, sentence_y_pred in zip(batch_X, batch_y_pred):\n",
    "                    if (\n",
    "                        sentence_X.count_nonzero() > 0\n",
    "                    ):  # Ignore the padding sentences\n",
    "                        batch_predictions.append(sentence_y_pred)\n",
    "                final_predictions.append(torch.mean(torch.stack(batch_predictions)))\n",
    "            final_y_pred = torch.stack(final_predictions)\n",
    "            return final_y_pred\n",
    "        else:\n",
    "            y_pred = self(X, pretrain)\n",
    "            return y_pred\n",
    " \n",
    "\n",
    "    @classmethod\n",
    "    def train_model(\n",
    "        cls: Type[T],\n",
    "        data: EyetrackingDataset,\n",
    "        min_epochs: int = 15,\n",
    "        max_epochs: int = 200,\n",
    "        dev_data: EyetrackingDataset = None,\n",
    "        device: str = \"cuda\",\n",
    "        config=None,\n",
    "        patience=10,\n",
    "        pretrained_model: T = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[T, int]:\n",
    "        model = pretrained_model or cls(data.num_features, config, **kwargs) #LSTMClassifier()\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "        epoch_count = 0\n",
    "        best_losses = [float(\"inf\")] * patience\n",
    "        for epoch in range(max_epochs):\n",
    "            # reshuffle data in each epoch\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                data,\n",
    "                batch_size=config[\"batch_size\"],\n",
    "                shuffle=True,\n",
    "                # drop_last=True\n",
    "            )\n",
    "            epoch_count += 1\n",
    "            epoch_loss = 0\n",
    "            for X, _, _, y in loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_pred = model._predict(X, subj_mean=BATCH_SUBJECTS).squeeze() # why do I need to squeeze here? I did not need in the simple model\n",
    "\n",
    "                loss = loss_fn(y_pred, y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()/len(X)  \n",
    "            mean_loss = epoch_loss/len(loader)\n",
    "            # print(f\"Epoch {epoch} done. Loss: {epoch_loss}\")\n",
    "            if dev_data is not None:\n",
    "                dev_accuracy = model.evaluate(dev_data, metric=\"loss\", device=device)\n",
    "                model.train()\n",
    "                # print(f\"Dev loss: {dev_accuracy} in Epoch {epoch}\")\n",
    "                if epoch > min_epochs and all(dev_accuracy > i for i in best_losses):\n",
    "                    epoch_count -= patience - best_losses.index(min(best_losses))\n",
    "                    break\n",
    "                else:\n",
    "                    best_losses.pop(0)\n",
    "                    best_losses.append(dev_accuracy)\n",
    "        return model\n",
    "\n",
    "    def predict_probs(\n",
    "        self,\n",
    "        data: EyetrackingDataset,\n",
    "        device: str = \"cuda\",\n",
    "        per_subj: bool = True,\n",
    "    ):\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        loader = torch.utils.data.DataLoader(data)\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        subjs = []\n",
    "        for X, _, subj, y in loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = self._predict(X, subj_mean=data.batch_subjects).squeeze()   # why do I need to squeeze here? I did not need in the simple model\n",
    "            \n",
    "            y_preds.append(y_pred.item())\n",
    "            y_trues.append(y.item())\n",
    "            subjs.append(subj)\n",
    "        if per_subj:\n",
    "            subjs, y_preds, y_trues = aggregate_speed_per_subject(\n",
    "                subjs, y_preds, y_trues\n",
    "            )\n",
    "        return y_preds, y_trues, subjs\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        data: EyetrackingDataset,\n",
    "        metric: str = \"loss\",\n",
    "        print_report: bool = False,\n",
    "        save_errors: TextIO = None,\n",
    "        per_subj: bool = False,\n",
    "        device: str = \"cuda\",\n",
    "    ) -> Tuple[float, float, float, float]:\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        loader = torch.utils.data.DataLoader(data)\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        subjs = []\n",
    "        loss = 0\n",
    "        for X, _, subj, y in loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = self._predict(X, subj_mean=BATCH_SUBJECTS).squeeze()\n",
    "            batch_loss = loss_fn(y_pred, y.squeeze())\n",
    "            loss += batch_loss.item()/len(X)     \n",
    "            \n",
    "            y_preds.append(y_pred.item())  \n",
    "            y_trues.append(y.item()) \n",
    "            subjs.append(subj) \n",
    "        mean_loss = loss/len(loader)\n",
    "        if per_subj:\n",
    "            subjs, y_preds, y_trues = aggregate_speed_per_subject(\n",
    "                subjs, y_preds, y_trues\n",
    "            )\n",
    "#         if print_report:\n",
    "#             print(\n",
    "#                 metrics.classification_report(y_trues, y_preds_class, zero_division=0)\n",
    "#             )\n",
    "#         if save_errors is not None:\n",
    "#             for subj, y_pred, y_true in zip(subjs, y_preds,  y_trues):\n",
    "#                 if y_pred_class != y_true:\n",
    "#                     save_errors.write(f\"{subj},{y_pred},{y_true}\\n\")\n",
    "        if metric == \"loss\":\n",
    "            return mean_loss\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric '{metric}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22c5ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(RSClassifier):\n",
    "    def initialize_model(self, input_size: int, config):\n",
    "        self.lstm = nn.LSTM(input_size, config[\"lstm_hidden_size\"], batch_first=True, bidirectional=True)  #50\n",
    "        self.linear1 = nn.Linear(config[\"lstm_hidden_size\"], 10)  #self.linear1 = nn.Linear(config[\"lstm_hidden_size\"], 12)\n",
    "        self.linear2 = nn.Linear(10, 5)\n",
    "        self.linear3 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, pretrain: bool = False) -> torch.Tensor:\n",
    "        lstm_output, (lstm_hidden, lstm_cell) = self.lstm(input)\n",
    "        lstm_hidden = lstm_hidden.mean(0)\n",
    "        linear1_output = self.linear1(lstm_hidden)\n",
    "        if pretrain:\n",
    "            linear_output = linear1_output.squeeze(1)\n",
    "        else:\n",
    "            linear_output = self.linear3(self.linear2(linear1_output)).squeeze(1)  \n",
    "         #   linear_output = torch.sigmoid(self.linear2(linear1_output).squeeze(1))\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8acd4",
   "metadata": {},
   "source": [
    "### Running a simple version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bc37cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "hyperparameter_space = {\n",
    "    \"lstm\": {\n",
    "        \"batch_size\": [64, 64],           #8, 16, 32, 64, 128\n",
    "        \"lr\": [1e-02, 1e-03, 1e-04],     #np.linspace(1e-5, 1e-1, num=15),    #loguniform.rvs(1e-5, 1e-1, size=15)\n",
    "        \"lstm_hidden_size\": [30, 40, 50],       #[10, 20, 30, 40, 50, 60, 70]\n",
    "        \"decision_boundary\": [0.5, 0.5]      #np.random.uniform(0, 1, size=20)\n",
    "    }\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    \"lstm\": {\n",
    "        \"epochs\": 40,\n",
    "        \"batch_size\": 32,\n",
    "        \"lr\": 0.001,\n",
    "        \"lstm_hidden_size\": 50,\n",
    "#        \"decision_boundary\": 0.5\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae755f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test fold  0\n",
      "dev fold 1\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 2\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 3\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 4\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 5\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 6\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 7\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 8\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 9\n",
      "tune set 0\n",
      "tune set 20\n",
      "test acc fold  0  :  0.21145648430489625\n",
      "test fold  1\n",
      "dev fold 0\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 2\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 3\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 4\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 5\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 6\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 7\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 8\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 9\n",
      "tune set 0\n",
      "tune set 20\n",
      "test acc fold  1  :  0.34758105273358525\n",
      "test fold  2\n",
      "dev fold 0\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 1\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 3\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 4\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 5\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 6\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 7\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 8\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 9\n",
      "tune set 0\n",
      "tune set 20\n",
      "test acc fold  2  :  0.25392237779832894\n",
      "test fold  3\n",
      "dev fold 0\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 1\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 2\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 4\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 5\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 6\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 7\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 8\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 9\n",
      "tune set 0\n",
      "tune set 20\n",
      "test acc fold  3  :  0.4918325670118191\n",
      "test fold  4\n",
      "dev fold 0\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 1\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 2\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 3\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 5\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 6\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 7\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 8\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 9\n",
      "tune set 0\n",
      "tune set 20\n",
      "test acc fold  4  :  0.34533726519996355\n",
      "test fold  5\n",
      "dev fold 0\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 1\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 2\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 3\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 4\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 6\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 7\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 8\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 9\n",
      "tune set 0\n",
      "tune set 20\n",
      "test acc fold  5  :  0.34849271031559337\n",
      "test fold  6\n",
      "dev fold 0\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 1\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 2\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 3\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 4\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 5\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 7\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 8\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 9\n",
      "tune set 0\n",
      "tune set 20\n",
      "test acc fold  6  :  0.7106744195214184\n",
      "test fold  7\n",
      "dev fold 0\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 1\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 2\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 3\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 4\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 5\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 6\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 8\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 9\n",
      "tune set 0\n",
      "tune set 20\n",
      "test acc fold  7  :  0.1811607744118411\n",
      "test fold  8\n",
      "dev fold 0\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 1\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 2\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 3\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 4\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 5\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 6\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 7\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 9\n",
      "tune set 0\n",
      "tune set 20\n",
      "test acc fold  8  :  0.45507165607755423\n",
      "test fold  9\n",
      "dev fold 0\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 1\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 2\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 3\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 4\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 5\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 6\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 7\n",
      "tune set 0\n",
      "tune set 20\n",
      "dev fold 8\n",
      "tune set 0\n",
      "tune set 20\n",
      "test acc fold  9  :  0.41047964872171483\n",
      "used test params:  [{'batch_size': 64, 'lr': 0.0001, 'lstm_hidden_size': 30, 'decision_boundary': 0.5}, {'batch_size': 64, 'lr': 0.001, 'lstm_hidden_size': 50, 'decision_boundary': 0.5}, {'batch_size': 64, 'lr': 0.01, 'lstm_hidden_size': 40, 'decision_boundary': 0.5}, {'batch_size': 64, 'lr': 0.001, 'lstm_hidden_size': 30, 'decision_boundary': 0.5}, {'batch_size': 64, 'lr': 0.001, 'lstm_hidden_size': 50, 'decision_boundary': 0.5}, {'batch_size': 64, 'lr': 0.01, 'lstm_hidden_size': 30, 'decision_boundary': 0.5}, {'batch_size': 64, 'lr': 0.0001, 'lstm_hidden_size': 50, 'decision_boundary': 0.5}, {'batch_size': 64, 'lr': 0.0001, 'lstm_hidden_size': 50, 'decision_boundary': 0.5}, {'batch_size': 64, 'lr': 0.001, 'lstm_hidden_size': 50, 'decision_boundary': 0.5}, {'batch_size': 64, 'lr': 0.01, 'lstm_hidden_size': 30, 'decision_boundary': 0.5}]\n",
      "mean: 0.3756008956096715 std: 0.0464012995370458\n"
     ]
    }
   ],
   "source": [
    "# default: --no-tune --wordvectors none --model cnn --subjpred\n",
    "parser = argparse.ArgumentParser(description=\"Run Russian Dyslexia Experiments\")\n",
    "parser.add_argument(\"--model\", dest=\"model\")\n",
    "parser.add_argument(\"--roc\", dest=\"roc\", action=\"store_true\")\n",
    "parser.add_argument(\"--no-roc\", dest=\"roc\", action=\"store_false\")\n",
    "parser.add_argument(\"--tunesets\", type=int, default=25)\n",
    "parser.add_argument(\"--tune\", dest=\"tune\", action=\"store_true\")\n",
    "parser.add_argument(\"--no-tune\", dest=\"tune\", action=\"store_false\")\n",
    "parser.add_argument(\"--wordvectors\", type=str, default=\"none\")\n",
    "parser.add_argument(\"--pretrain\", dest=\"pretrain\", action=\"store_true\")\n",
    "parser.add_argument(\"--subjpred\", dest=\"batch_subjects\", action=\"store_false\")\n",
    "parser.add_argument(\"--textpred\", dest=\"batch_subjects\", action=\"store_true\")\n",
    "parser.add_argument(\"--save-errors\", dest=\"save_errors\", type=argparse.FileType(\"w\"))\n",
    "parser.add_argument(\"--seed\", dest=\"seed\", type=int, default=43)\n",
    "parser.add_argument(\"--cuda\", dest=\"cudaid\", default=0)\n",
    "parser.set_defaults(tune=True) #True\n",
    "parser.set_defaults(roc=True)\n",
    "parser.set_defaults(batch_subjects=True) #True\n",
    "parser.set_defaults(model = \"lstm\")\n",
    "args = parser.parse_args(args=[]) # modified to work with jupyter notebook\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "args\n",
    "\n",
    "if args.model == \"lstm\":\n",
    "    MODEL_CLASS = LSTMClassifier\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    device = torch.device(f'cuda:{args.cudaid}')\n",
    "    \n",
    "NUM_FOLDS = 10\n",
    "NUM_TUNE_SETS = args.tunesets\n",
    "BATCH_SUBJECTS = args.batch_subjects\n",
    "tune = args.tune\n",
    "\n",
    "\n",
    "if args.save_errors is not None:\n",
    "    args.save_errors.write(\"subj,y_pred,y_true\\n\")\n",
    "\n",
    "if tune:\n",
    "    used_test_params = []\n",
    "    parameter_sample = [\n",
    "        get_params(hyperparameter_space[args.model]) for _ in range(NUM_TUNE_SETS)\n",
    "    ]\n",
    "\n",
    "    \n",
    "tprs_folds = {}\n",
    "    \n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# load and preprocess data for training\n",
    "preprocessor = EyetrackingDataPreprocessor(\n",
    "    csv_file = 'data/fixation_dataset_long_no_word_padded_word_pos.csv', # 'data/fixation_dataset_30fix_padded.csv',  \n",
    "    num_folds = NUM_FOLDS\n",
    ")\n",
    "\n",
    "\n",
    "test_accuracies = []\n",
    "for test_fold in range(NUM_FOLDS):\n",
    "    #test_fold = 5   # Triple cross-validation, test fold stays the same\n",
    "    print(\"test fold \", test_fold)\n",
    "    parameter_evaluations = np.zeros(shape=(NUM_FOLDS, NUM_TUNE_SETS))\n",
    "    if tune:\n",
    "        # Normal training / fine-tuning\n",
    "        for dev_fold in range(NUM_FOLDS):\n",
    "            if args.pretrain:\n",
    "                pretrained_models = next(pretrained_model_generator)\n",
    "            if dev_fold == test_fold:\n",
    "                continue\n",
    "            print(f'dev fold {dev_fold}')\n",
    "            train_folds = [\n",
    "                fold\n",
    "                for fold in range(NUM_FOLDS)\n",
    "                if fold != test_fold and fold != dev_fold\n",
    "            ]\n",
    "    #             word_vector_model = preprocessor.train_word_vector_model(\n",
    "    #                     folds=train_folds\n",
    "    #                 )\n",
    "            # When fine-tuning, we use the pre-trained word vector model (?)\n",
    "            train_dataset = EyetrackingDataset(\n",
    "                preprocessor,\n",
    "                folds=train_folds,\n",
    "                batch_subjects=BATCH_SUBJECTS,\n",
    "            )\n",
    "            mean, sd = getmeansd(train_dataset, batch=BATCH_SUBJECTS)\n",
    "            train_dataset.standardize(mean, sd)\n",
    "            dev_dataset = EyetrackingDataset(\n",
    "                preprocessor,\n",
    "                folds=[dev_fold],\n",
    "                batch_subjects=BATCH_SUBJECTS,\n",
    "            )\n",
    "            dev_dataset.standardize(mean, sd)\n",
    "            for tune_set in range(NUM_TUNE_SETS):\n",
    "                running_model = copy.deepcopy(MODEL_CLASS)\n",
    "                if tune_set%20 == 0:\n",
    "                    print(f'tune set {tune_set}')\n",
    "                if args.pretrain:\n",
    "                    pretrained_model = next(pretrained_models)\n",
    "                else:\n",
    "                    pretrained_model = None\n",
    "                model = None\n",
    "                gc.collect()\n",
    "                model = running_model.train_model(\n",
    "                    train_dataset,\n",
    "                    min_epochs=15,\n",
    "                    max_epochs=200,\n",
    "                    dev_data=dev_dataset,\n",
    "                    pretrained_model=pretrained_model,\n",
    "                    device=device,\n",
    "                    config=parameter_sample[tune_set],\n",
    "                )\n",
    "                tune_accuracy = model.evaluate(\n",
    "                    data=dev_dataset,\n",
    "                    device=device,\n",
    "                    metric=\"loss\",\n",
    "                    per_subj=BATCH_SUBJECTS,\n",
    "                )\n",
    "                parameter_evaluations[dev_fold, tune_set] = tune_accuracy\n",
    "        # Select best parameter set\n",
    "        mean_dev_accuracies = np.mean(parameter_evaluations, axis=0)\n",
    "        best_parameter_set = np.argmax(mean_dev_accuracies)\n",
    "        params_test = parameter_sample[best_parameter_set]\n",
    "        # print(f'best performing parameter for fold ', test_fold, \": \", params_test)\n",
    "        used_test_params.append(params_test)\n",
    "        if args.pretrain:\n",
    "            pretrained_model = copy.deepcopy(MODEL_CLASS)\n",
    "            best_pretrained_model = pretrained_model.pretrain_model(\n",
    "                        pretrain_dataset,\n",
    "                        epochs=100,\n",
    "                        device=device,\n",
    "                        config=params_test,\n",
    "                    )\n",
    "        else:\n",
    "            best_pretrained_model = None\n",
    "    else:  # (not tuning)\n",
    "        params_test = default_params[args.model]\n",
    "        best_pretrained_model = None\n",
    "    # If tune: train using best feature set over dev sets, else: train using default parameters\n",
    "    # Use fold next to test fold for early stopping\n",
    "    running_model = copy.deepcopy(MODEL_CLASS)\n",
    "    dev_fold = (test_fold + 1) % NUM_FOLDS\n",
    "    train_folds = [\n",
    "        fold for fold in range(NUM_FOLDS) if fold != test_fold and fold != dev_fold\n",
    "    ]\n",
    "    #word_vector_model = preprocessor.train_word_vector_model(folds=train_folds)\n",
    "    train_dataset = EyetrackingDataset(\n",
    "        preprocessor,\n",
    "        folds=train_folds,\n",
    "        batch_subjects=BATCH_SUBJECTS,\n",
    "    )\n",
    "    mean, sd = getmeansd(train_dataset, batch=BATCH_SUBJECTS)\n",
    "    train_dataset.standardize(mean, sd)\n",
    "    dev_dataset = EyetrackingDataset(\n",
    "        preprocessor,\n",
    "        folds=[dev_fold],\n",
    "        batch_subjects=BATCH_SUBJECTS\n",
    "    )\n",
    "    dev_dataset.standardize(mean, sd)\n",
    "    test_dataset = EyetrackingDataset(\n",
    "        preprocessor,\n",
    "        folds=[test_fold],\n",
    "        batch_subjects=BATCH_SUBJECTS,\n",
    "    )\n",
    "    test_dataset.standardize(mean, sd)\n",
    "    model = running_model.train_model(\n",
    "        train_dataset,\n",
    "        min_epochs=15,\n",
    "        max_epochs=200,\n",
    "        dev_data=dev_dataset,\n",
    "        pretrained_model=best_pretrained_model,\n",
    "        device=device,\n",
    "        config=params_test,\n",
    "    )\n",
    "    #print(f'test accuraccy fold ', test_fold)\n",
    "    test_accuracy = model.evaluate(\n",
    "        test_dataset,\n",
    "        device=device,\n",
    "        metric=\"loss\",\n",
    "        print_report=True,\n",
    "        per_subj=BATCH_SUBJECTS,\n",
    "        save_errors=args.save_errors,\n",
    "    )\n",
    "    print(\"test acc fold \", test_fold, \" : \", test_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    torch.save(model.state_dict(), f'saved_models/test/rs_nested_cv/test_fold_{test_fold}')\n",
    "    if True:\n",
    "        y_preds, y_trues, subjs = model.predict_probs(\n",
    "            test_dataset,\n",
    "            device=device,\n",
    "            per_subj=BATCH_SUBJECTS,\n",
    "        )\n",
    "        tprs_folds[str(test_fold)] = (y_trues, y_preds, subjs)\n",
    "#         Roc.get_tprs_aucs(y_trues, y_preds, test_fold)\n",
    "\n",
    "\n",
    "if tune:\n",
    "    print(\"used test params: \", used_test_params)\n",
    "print(\n",
    "    \"mean:\",\n",
    "    np.mean(test_accuracies, axis=0),\n",
    "    \"std:\",\n",
    "    np.std(test_accuracies, axis=0) / np.sqrt(NUM_FOLDS),\n",
    ")\n",
    "\n",
    "# if args.roc:\n",
    "#     Roc.plot()\n",
    "#     Roc.save()\n",
    "#     print(\"auc: \", Roc.mean_auc, \"+-\", Roc.std_auc)\n",
    "pred_level = \"subjectpred\" if BATCH_SUBJECTS else \"textpred\"\n",
    "final_scores_mean = np.mean(test_accuracies, axis=0)\n",
    "final_scores_std = np.std(test_accuracies, axis=0) / np.sqrt(NUM_FOLDS)\n",
    "# final_scores_mean = np.insert(final_scores_mean, 0, Roc.mean_auc, axis=0)\n",
    "# final_scores_std = np.insert(final_scores_std, 0, Roc.std_auc, axis=0)\n",
    "# out_str = \"\"\n",
    "# with open(f\"reader_prediction_results/{args.model}_scores_{pred_level}.txt\", \"w\") as f:\n",
    "#     for i in range(len(final_scores_mean)):\n",
    "#         out_str += f\"${round(final_scores_mean[i],2):1.2f}\\db{{{round(final_scores_std[i],2):1.2f}}}$\"\n",
    "#         if i < len(final_scores_mean) - 1:\n",
    "#             out_str += \" & \"\n",
    "#         else:\n",
    "#             out_str += \" \\\\\\\\ \"\n",
    "#     f.write(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f929863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'batch_size': 64,\n",
       "  'lr': 0.0001,\n",
       "  'lstm_hidden_size': 30,\n",
       "  'decision_boundary': 0.5},\n",
       " {'batch_size': 64,\n",
       "  'lr': 0.001,\n",
       "  'lstm_hidden_size': 50,\n",
       "  'decision_boundary': 0.5},\n",
       " {'batch_size': 64,\n",
       "  'lr': 0.01,\n",
       "  'lstm_hidden_size': 40,\n",
       "  'decision_boundary': 0.5},\n",
       " {'batch_size': 64,\n",
       "  'lr': 0.001,\n",
       "  'lstm_hidden_size': 30,\n",
       "  'decision_boundary': 0.5},\n",
       " {'batch_size': 64,\n",
       "  'lr': 0.001,\n",
       "  'lstm_hidden_size': 50,\n",
       "  'decision_boundary': 0.5},\n",
       " {'batch_size': 64,\n",
       "  'lr': 0.01,\n",
       "  'lstm_hidden_size': 30,\n",
       "  'decision_boundary': 0.5},\n",
       " {'batch_size': 64,\n",
       "  'lr': 0.0001,\n",
       "  'lstm_hidden_size': 50,\n",
       "  'decision_boundary': 0.5},\n",
       " {'batch_size': 64,\n",
       "  'lr': 0.0001,\n",
       "  'lstm_hidden_size': 50,\n",
       "  'decision_boundary': 0.5},\n",
       " {'batch_size': 64,\n",
       "  'lr': 0.001,\n",
       "  'lstm_hidden_size': 50,\n",
       "  'decision_boundary': 0.5},\n",
       " {'batch_size': 64,\n",
       "  'lr': 0.01,\n",
       "  'lstm_hidden_size': 30,\n",
       "  'decision_boundary': 0.5}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_test_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e46670f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('{\"batch_size\": 64, \"decision_boundary\": 0.5, \"lr\": 0.001, \"lstm_hidden_size\": 50}',\n",
       " 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find if there is a winning combination.\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "json_dicts = [json.dumps(d, sort_keys=True) for d in used_test_params]\n",
    "\n",
    "# Count duplicates\n",
    "counts = Counter(json_dicts)\n",
    "\n",
    "# There is:\n",
    "max_entry = max(counts.items(), key=lambda x: x[1])\n",
    "max_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e69c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = []\n",
    "subjects = []\n",
    "\n",
    "for fold in range(len(preprocessor._folds)):\n",
    "    fold_column = np.full(len(preprocessor._folds[fold]), fold)\n",
    "    folds.extend(fold_column)\n",
    "    subjects.extend(preprocessor._folds[fold])\n",
    "\n",
    "subj_folds = pd.DataFrame({'fold':folds, 'subject':subjects})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877921de",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = []\n",
    "groups = []\n",
    "pred_probs = []\n",
    "subjs = []\n",
    "\n",
    "for fold in tprs_folds:\n",
    "    fold_column = np.full(len(tprs_folds[fold][0]), int(fold))\n",
    "    folds.extend(fold_column)\n",
    "    groups.extend(tprs_folds[fold][0])\n",
    "    pred_probs.extend(tprs_folds[fold][1])\n",
    "    subjs.extend(tprs_folds[fold][2])\n",
    "\n",
    "pred_folds = pd.DataFrame({'fold':folds, 'group':groups, 'pred_prob':pred_probs, 'subject':subjs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6880c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subj_folds.merge(pred_folds, on='subject')\n",
    "df = df.drop(['fold_x'], axis=1)\n",
    "df = df.rename(columns={'fold_y': \"fold\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ade0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = pd.read_csv('data/demo_filtered_centered.csv', decimal=\",\")\n",
    "demo = demo.rename(columns={'subj_demo': \"subject\"})\n",
    "len(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf98a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = df.merge(demo, on =['subject'])\n",
    "final.to_csv('rs_predictions_full_info.csv', index=False)\n",
    "len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BATCH_SUBJECTS:\n",
    "    setting = \"reader\"\n",
    "    folder = \"reader_prediction_results/predicting_reading_speed/\"\n",
    "else:\n",
    "    setting = \"sentence\"\n",
    "    folder = \"sentence_prediction_results/\"\n",
    "    \n",
    "\n",
    "with open(f'{folder}folds_{setting}.pickle', 'wb') as handle:\n",
    "    pickle.dump(preprocessor._folds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'{folder}tpr_folds_{setting}.pickle', 'wb') as handle:\n",
    "    pickle.dump(tprs_folds, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'{folder}final_scores_mean_{setting}.pickle', 'wb') as handle:\n",
    "    pickle.dump(final_scores_mean, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(f'{folder}test_accs_{setting}.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_accuracies, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(f'{folder}test_params_{setting}.pickle', 'wb') as handle: \n",
    "    pickle.dump(used_test_params, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4a56a",
   "metadata": {},
   "source": [
    "## Part 2 - verifying that I can now predict reading speed using the best parameter values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier_RS(RSClassifier):\n",
    "    def initialize_model(self, input_size: int, config):\n",
    "        self.lstm = nn.LSTM(input_size, config[\"lstm_hidden_size\"], batch_first=True, bidirectional=True)  #50\n",
    "        self.linear1 = nn.Linear(config[\"lstm_hidden_size\"], 10) \n",
    "        self.linear2 = nn.Linear(10, 5)\n",
    "        self.linear3 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, pretrain: bool = False) -> torch.Tensor:\n",
    "        lstm_output, (lstm_hidden, lstm_cell) = self.lstm(input)\n",
    "        lstm_hidden = lstm_hidden.mean(0)\n",
    "        linear1_output = self.linear1(lstm_hidden)\n",
    "        if pretrain:\n",
    "            linear_output = linear1_output.squeeze(1)\n",
    "        else:\n",
    "            linear_output = self.linear3(self.linear2(linear1_output)).squeeze(1)  \n",
    "         #   linear_output = torch.sigmoid(self.linear2(linear1_output).squeeze(1))\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d66193",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test_rs = {\"batch_size\": 64, \"decision_boundary\": 0.5, \"lr\": 0.001, \"lstm_hidden_size\": 50}\n",
    "max_epochs = 120\n",
    "min_epochs = 15\n",
    "patience = 20\n",
    "NUM_FOLDS = 10\n",
    "\n",
    "preprocessor = EyetrackingDataPreprocessor(csv_file='data/fixation_dataset_long_no_word_padded_word_pos.csv',\n",
    "                                               num_folds = NUM_FOLDS)\n",
    "\n",
    "# Setup loss function\n",
    "loss_fn_rs = nn.MSELoss()\n",
    "\n",
    "BATCH_SUBJECTS = True\n",
    "running_model = copy.deepcopy(LSTMClassifier_RS)\n",
    "tprs_folds_rs = {}\n",
    "\n",
    "for test_fold in range(NUM_FOLDS):\n",
    "    print(\"test fold \", test_fold)\n",
    "    dev_fold = (test_fold + 1) % NUM_FOLDS\n",
    "    train_folds = [\n",
    "        fold for fold in range(NUM_FOLDS) if fold != test_fold and fold != dev_fold\n",
    "    ]\n",
    "    train_dataset = EyetrackingDataset(\n",
    "        preprocessor,\n",
    "        folds=train_folds,\n",
    "        batch_subjects=BATCH_SUBJECTS,\n",
    "    )\n",
    "    mean, sd = getmeansd(train_dataset, batch=BATCH_SUBJECTS)\n",
    "    train_dataset.standardize(mean, sd)\n",
    "    dev_dataset = EyetrackingDataset(\n",
    "        preprocessor,\n",
    "        folds=[dev_fold],\n",
    "        batch_subjects=BATCH_SUBJECTS\n",
    "    )\n",
    "    dev_dataset.standardize(mean, sd)\n",
    "    test_dataset = EyetrackingDataset(\n",
    "        preprocessor,\n",
    "        folds=[test_fold],\n",
    "        batch_subjects=BATCH_SUBJECTS,\n",
    "    )\n",
    "    test_dataset.standardize(mean, sd)\n",
    "    model = running_model.train_model(\n",
    "        train_dataset,\n",
    "        min_epochs=15,\n",
    "        max_epochs=200,\n",
    "        dev_data=dev_dataset,\n",
    "      #  pretrained_model=best_pretrained_model,\n",
    "        device=device,\n",
    "        config=params_test_rs,\n",
    "    )\n",
    "    y_preds, y_trues, subjs = model.predict_probs(\n",
    "        test_dataset,\n",
    "        device=device,\n",
    "        per_subj=BATCH_SUBJECTS,\n",
    "    )\n",
    "    tprs_folds_rs[str(test_fold)] = (y_trues, y_preds, subjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10df64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = []\n",
    "pred_speed = []\n",
    "subjs = []\n",
    "\n",
    "for fold in tprs_folds_rs:\n",
    "   # y_trues.extend(tprs_folds[fold][0])\n",
    "    pred_speed.extend(tprs_folds[fold][1])\n",
    "    subjs.extend(tprs_folds[fold][2])\n",
    "\n",
    "reading_speed = pd.DataFrame({'Reading_speed':pred_speed, 'subj':subjs})\n",
    "reading_speed = reading_speed.set_index('subj')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

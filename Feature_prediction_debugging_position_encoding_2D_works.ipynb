{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444f024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 14:40:03.627254: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-16 14:40:03.691277: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-16 14:40:03.709463: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "#from IPython.display import set_matplotlib_formats\n",
    "#matplotlib_inline.backend_inline.set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "# ## tqdm for loading bars\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# for one-hot encoding\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from mlm_pytorch import MLM\n",
    "\n",
    "# Transformer wrapper\n",
    "import tensorflow as tf\n",
    "from x_transformers import TransformerWrapper, Encoder\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# Positional encoding in two dimensions\n",
    "from positional_encodings.torch_encodings import PositionalEncodingPermute1D, PositionalEncoding1D\n",
    "\n",
    "from functools import reduce\n",
    "import math\n",
    "\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.4\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Path to the folder where the datasets are\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"saved_models/simple_transformer\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66f8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.5\n",
    "    import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21cf40",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91aec7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "NUM_FEATURES = 1\n",
    "NUM_FIX = 30 \n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 31\n",
    "\n",
    "\n",
    "num_fix = NUM_FIX\n",
    "seq_len = NUM_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a1ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPositionalEncoding(nn.Module):\n",
    "    \"\"\"Learnable positional encoding for features \"\"\"\n",
    "    def __init__(self, embed_dim, max_len=1):\n",
    "        super(CustomPositionalEncoding, self).__init__()\n",
    "\n",
    "    # Initialize a learnable positional encoding matrix\n",
    "        self.encoding = nn.Parameter(torch.zeros(max_len, embed_dim)).to(device)\n",
    "        nn.init.xavier_uniform_(self.encoding)  # Xavier initialization for better training stability\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        if mask is not None:\n",
    "            # Apply the mask to ignore padded positions\n",
    "            pos_encoding = self.encoding[:x.size(1), :] * mask\n",
    "    # Add the learnable positional encoding to the input tensor\n",
    "        return x + pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dc765cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim = 30\n",
    "# encoding = nn.Parameter(torch.zeros(1, embed_dim)).to(device)\n",
    "# nn.init.xavier_uniform_(encoding)\n",
    "# mask = ~no_mask\n",
    "\n",
    "# pos_encoding = encoding[:seq.size(1), :] * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8823925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "    \n",
    "# def mse_loss(target, input, ignored_index):\n",
    "#     mask = target == ignored_index\n",
    "#     out = (input[~mask]-target[~mask])**2\n",
    "#     return out.mean()\n",
    "\n",
    "def mse_loss(target, input, mask):\n",
    "    out = (input[mask]-target[mask])**2\n",
    "    return out.mean()\n",
    "\n",
    "def mask_with_tokens_3D(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    reduced = torch.any(mask, dim=-1, keepdim=True)\n",
    "    expanded = reduced.expand_as(mask)\n",
    "    return expanded\n",
    "\n",
    "def get_mask_subset_with_prob_3D(mask, prob):\n",
    "    batch, num_fix, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * num_fix)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-2, keepdim=True)\n",
    "    mask_excess = (mask.cumsum(dim=-2)[:,:,0] > (num_tokens[:,:,0] * prob).ceil())\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, num_fix, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-2)\n",
    "    sampled_indices = (sampled_indices[:,:,0] + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, num_fix + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    new_mask = new_mask[:, 1:].bool()\n",
    "    \n",
    "    return new_mask.unsqueeze_(2).expand(-1,-1, seq_len)\n",
    "    \n",
    "def get_mask_subset_with_prob(mask, prob):\n",
    "    batch, seq_len, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * seq_len)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-1, keepdim=True)\n",
    "    mask_excess = (mask.cumsum(dim=-1) > (num_tokens * prob).ceil())\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, seq_len), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-1)\n",
    "    sampled_indices = (sampled_indices + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, seq_len + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    return new_mask[:, 1:].bool()\n",
    "\n",
    "def prob_mask_like(t, prob):\n",
    "    return torch.zeros_like(t).float().uniform_(0, 1) < prob\n",
    "\n",
    "def prob_mask_like_3D(t, prob):\n",
    "    temp = torch.zeros_like(t[:,:,0]).float().uniform_(0, 1) < prob\n",
    "    return temp.unsqueeze_(2).expand(-1,-1, seq_len)\n",
    "\n",
    "    \n",
    "# class CustomPositionalEncoding(nn.Module):\n",
    "#     \"\"\"Learnable positional encoding for both features and fixations\"\"\"\n",
    "#     def __init__(self, fixations, embed_dim, max_len=5000):\n",
    "#         super(CustomPositionalEncoding, self).__init__()\n",
    "        \n",
    "#         # Initialize a learnable positional encoding matrix\n",
    "#         self.encoding = nn.Parameter(torch.zeros(fixations, embed_dim))\n",
    "#         nn.init.xavier_uniform_(self.encoding)  # Xavier initialization for better training stability\n",
    "\n",
    "#     def forward(self, x, mask = None):\n",
    "#         if mask is not None:\n",
    "#             # Apply the mask to ignore padded positions\n",
    "#             pos_encoding = self.encoding * mask\n",
    "#         # Assumes input `x` is of shape [batch_size, fixations, embed_dim]\n",
    "#         return x + pos_encoding#.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    \n",
    "def pad_group_with_zeros(group, target_rows):\n",
    "    # Calculate the number of rows to add\n",
    "    num_missing_rows = target_rows - len(group)\n",
    "    if num_missing_rows > 0:\n",
    "        # Create a DataFrame with the required number of padding rows\n",
    "        # input padding\n",
    "        zero_rows = pd.DataFrame(0.3333, index=range(num_missing_rows), columns=group.columns)\n",
    "        # Label padding\n",
    "        # zero_rows.iloc[:, 0] = 31\n",
    "        # Concatenate the group with the zero rows\n",
    "        group = pd.concat([group, zero_rows], ignore_index=True)\n",
    "    return group\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert Series in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        trial, label = sample['trial'], sample['label']\n",
    "        trial = torch.from_numpy(trial).float()\n",
    "        label = torch.from_numpy(label).float()\n",
    "        return trial, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e747c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSCFixationsOrder(Dataset):\n",
    "    \"\"\"Dataset with the long-format sequence of fixations made during reading by dyslexic \n",
    "    and normally-developing Russian-speaking monolingual children.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None, target_transform = None, \n",
    "                 n_fix = NUM_FIX, \n",
    "                 dropPhonologyFeatures = True, dropPhonologySubjects = True):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied\n",
    "                on a label.\n",
    "        \"\"\"\n",
    "        self.fixations_frame = pd.read_csv(csv_file)\n",
    "        \n",
    "        # remove demography and identification\n",
    "        self.fixations_frame = self.fixations_frame.drop(columns = ['fix_x', \n",
    "                                                                    'fix_y', 'full_text' \n",
    "                                                                    ]) \n",
    "        \n",
    "        #self.fixations_frame = self.fixations_frame[order_cols]\n",
    "        \n",
    "        # Log-transforming appropriate measures\n",
    "        to_transform = ['frequency', 'fix_dur'] \n",
    "        for column in to_transform:\n",
    "            self.fixations_frame[column] = self.fixations_frame[column].apply(lambda x: np.log(x) if x > 0 else 0) \n",
    "\n",
    "        # Center \n",
    "        cols = ['fix_dur', 'landing', 'predictability',\n",
    "                'frequency', 'word_length', 'number.morphemes', \n",
    "                'next_fix_dist', 'sac_ampl', 'sac_angle', \n",
    "                'sac_vel']\n",
    "        for col in cols:\n",
    "            self.fixations_frame[col] = np.where(self.fixations_frame[col] == 0, -4,\n",
    "                (self.fixations_frame[col] - self.fixations_frame[col].mean())/self.fixations_frame[col].std(ddof=0)) \n",
    "        \n",
    "\n",
    "        # Drop padding \n",
    "        self.fixations_frame = self.fixations_frame[self.fixations_frame['fix_dur'] != -4]\n",
    "        \n",
    "        \n",
    "        # Convert direction to a dummy-coded variable\n",
    "        self.fixations_frame['direction'] = np.where(self.fixations_frame['direction'].isnull(), 0,\n",
    "                                                     self.fixations_frame['direction'])\n",
    "        self.fixations_frame = pd.concat([self.fixations_frame, \n",
    "                                          pd.get_dummies(self.fixations_frame['direction'], \n",
    "                                                         prefix='dummy')], axis=1)\n",
    "        \n",
    "        # Center exceptions: \n",
    "        # columns where 0 is meaningful and should not be counted as NA\n",
    "#         cols = ['rel.position', 'dummy_DOWN', 'dummy_LEFT','dummy_RIGHT', 'dummy_UP']\n",
    "#         for col in cols:\n",
    "#             self.fixations_frame[col] = (self.fixations_frame[col] - \\\n",
    "#                                              self.fixations_frame[col].mean())/self.fixations_frame[col].std(ddof=0)\n",
    "            \n",
    "        if dropPhonologySubjects == True:\n",
    "            # Drop subjects\n",
    "            self.fixations_frame.dropna(axis = 0, how = 'any', inplace = True)\n",
    "        else:\n",
    "            # Drop columns\n",
    "            self.fixations_frame.dropna(axis = 1, how = 'any', inplace = True)\n",
    "        \n",
    "        \n",
    "        self.fixations_frame['subj'] = self.fixations_frame['subj'].astype(str)\n",
    "        self.fixations_frame['item'] = self.fixations_frame['sn'].astype(str)\n",
    "        self.fixations_frame['Combined'] = self.fixations_frame[['subj', 'item']].agg('_'.join, axis=1)\n",
    "        \n",
    "        # cleaning up\n",
    "        self.fixations_frame.drop(columns = ['subj', 'item', 'direction', 'dummy_0', 'sn',\\\n",
    "                                            'dummy_DOWN', 'dummy_LEFT', 'dummy_UP'\\\n",
    "                                            ], inplace = True)\n",
    "        self.fixations_frame.drop(columns = ['word_length', 'predictability', \n",
    "                                             'frequency', 'number.morphemes', 'fix_index'], \n",
    "                                             inplace = True)\n",
    "        self.fixations_frame.drop(columns = ['landing', \n",
    "                'next_fix_dist', 'sac_ampl', 'sac_angle', \n",
    "                'sac_vel', 'dummy_RIGHT'], inplace = True)\n",
    "        \n",
    "\n",
    "        padded = self.fixations_frame.groupby('Combined', group_keys=False).apply(lambda x: \n",
    "                                                                           pad_group_with_zeros(x, n_fix))\n",
    "        padded.drop(columns = \"Combined\", inplace = True)\n",
    "    \n",
    "        \n",
    "        #### Fixation index is the label\n",
    "        self.fixations_frame = padded.to_numpy()\n",
    "        dataReshaped = np.reshape(self.fixations_frame, (int(len(self.fixations_frame)/n_fix), \n",
    "                                                         n_fix, self.fixations_frame.shape[1]))\n",
    "\n",
    "        self.predictors = dataReshaped[:,:,1:]\n",
    "        self.labels = dataReshaped[:,:,0]\n",
    "        self.labels = to_categorical(self.labels-1, num_classes=NUM_CLASSES)   # one-hot encoding\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        trial = self.predictors[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        sample = {'trial': trial, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        if self.target_transform:\n",
    "            sample = self.target_transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8466dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = RSCFixationsOrder(csv_file='data/RSC_long_no_word_padded_word_pos.csv', \n",
    "                                    transform=ToTensor(),\n",
    "                                    dropPhonologySubjects = True)\n",
    "\n",
    "train_size = int(0.8 * len(transformed_dataset))\n",
    "val_size = int(0.1 * len(transformed_dataset))\n",
    "test_size = len(transformed_dataset) - val_size - train_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(transformed_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511ba45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 30, 1])\n",
      "Labels batch shape: torch.Size([64, 30, 31])\n"
     ]
    }
   ],
   "source": [
    "trials, labels = next(iter(train_loader))\n",
    "\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa35291a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.1000],\n",
       "        [0.2000],\n",
       "        [0.1000],\n",
       "        [0.3000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.6000],\n",
       "        [0.7000],\n",
       "        [0.9000],\n",
       "        [0.8000],\n",
       "        [0.3333],\n",
       "        [0.3333],\n",
       "        [0.3333],\n",
       "        [0.3333]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials[1, 0:15,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508254d",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7d60f",
   "metadata": {},
   "source": [
    "-- Instead of reconstructing the vocabulary, we would use MSE and try to reconstruct *x, y, fix dur* and we can look into how to reconstruct the rest of the features (i.e. all the additional that you used).\n",
    "\n",
    "-- and an adjustable pytorch implementation here:\n",
    "https://github.com/lucidrains/mlm-pytorch/tree/master\n",
    "\n",
    "-- https://github.com/lucidrains/mlm-pytorch\n",
    "\n",
    "-- Embedding for position - DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "162735ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len, l2norm_embed = l2norm_embed)\n",
    "\n",
    "# emb_dim = 7\n",
    "# max_seq_len = 30\n",
    "\n",
    "# pos_emb = AbsolutePositionalEmbedding(seq_len, num_fix, l2norm_embed = l2norm_embed)\n",
    "# pos_emb(X_train)\n",
    "\n",
    "# # #X_train + pos_emb(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3a5b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc58dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 30])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.squeeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7045bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annas changes\n",
    "class TransformerWithCustomPositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim, \n",
    "        num_heads, \n",
    "        num_layers, \n",
    "        max_len=5000,\n",
    "        mask_prob = 0.15,\n",
    "        replace_prob = 1, # 0.9\n",
    "        mask_token_id = 2,\n",
    "        pad_token_id = 0.3333,\n",
    "        mask_ignore_token_ids = []\n",
    "        ):\n",
    "        super(TransformerWithCustomPositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.mask_prob = mask_prob\n",
    "        self.replace_prob = replace_prob\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, pad_token_id])\n",
    "        \n",
    "        # positional encoding\n",
    "        #self.positional_encoding = PositionalEncoding(d_model = embed_dim)\n",
    "        self.positional_encoding = CustomPositionalEncoding(num_fix).to(device)\n",
    "        #self.positional_encoding = AbsolutePositionalEmbedding(seq_len, num_fix, l2norm_embed = l2norm_embed)\n",
    "        \n",
    "        # transformer\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                                        nhead=num_heads, \n",
    "                                                        batch_first = True)\n",
    "        self.transformer = TransformerEncoder(self.encoder_layer, num_layers=num_layers).to(device)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        seq = seq.squeeze(2)\n",
    "        \n",
    "        no_mask = mask_with_tokens(seq, self.mask_ignore_token_ids) \n",
    "        mask = get_mask_subset_with_prob(~no_mask, self.mask_prob)\n",
    "\n",
    "        # mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "        masked_seq = seq.clone().detach()\n",
    "        masked_seq_pos = self.positional_encoding(masked_seq, mask = ~no_mask)\n",
    "\n",
    "        # [mask] input\n",
    "        masked_replace_prob = prob_mask_like(seq, self.replace_prob) # Anna: select 90% of all values  (ignore all masking for now)\n",
    "        masked_seq = masked_seq_pos.masked_fill(mask * masked_replace_prob, self.mask_token_id) # Anna: select 90% only of those selected for masking\n",
    "        \n",
    "        # derive labels to predict\n",
    "        labels = seq \n",
    "        labels = labels.masked_fill(~mask, self.pad_token_id)\n",
    "        \n",
    "        # Pass through the transformer\n",
    "        preds = self.transformer(masked_seq)\n",
    "        \n",
    "        my_loss = mse_loss(\n",
    "            labels,\n",
    "            preds,\n",
    "            #ignored_index = self.pad_token_id\n",
    "            mask = mask\n",
    "        )\n",
    "\n",
    "        return preds, my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a63406ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = trials.squeeze(2)\n",
    "seq = X_train\n",
    "\n",
    "mask_prob = 0.15\n",
    "replace_prob = 1\n",
    "mask_token_id = 2\n",
    "pad_token_id = 0.3333\n",
    "mask_ignore_token_ids = set([pad_token_id])\n",
    "embed_dim = num_fix\n",
    "\n",
    "\n",
    "positional_encoding = CustomPositionalEncoding(num_fix).to(device)\n",
    "#positional_encoding = AbsolutePositionalEmbedding(seq_len, num_fix, l2norm_embed = False)\n",
    "\n",
    "# transformer\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, \n",
    "                                            nhead=1, \n",
    "                                            batch_first = True)\n",
    "transformer = TransformerEncoder(encoder_layer, num_layers=2).to(device)\n",
    "\n",
    "no_mask = mask_with_tokens(seq, mask_ignore_token_ids) # works in 3D\n",
    "mask = get_mask_subset_with_prob(~no_mask, mask_prob)\n",
    "\n",
    "# mask input with mask tokens with probability of `replace_prob` (keep tokens the same with probability 1 - replace_prob)\n",
    "masked_seq = seq.clone().detach()\n",
    "masked_seq_pos = positional_encoding(masked_seq, mask = ~no_mask)\n",
    "\n",
    "# [mask] input\n",
    "masked_replace_prob = prob_mask_like(seq, replace_prob) # Anna: select 90% of all values  (ignore all masking for now)\n",
    "masked_seq1 = masked_seq_pos.masked_fill(mask * masked_replace_prob, mask_token_id) # Anna: select 90% only of those selected for masking\n",
    "\n",
    "labels = seq \n",
    "labels = labels.masked_fill(~mask, pad_token_id)\n",
    "\n",
    "preds = transformer(masked_seq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2f47190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0000,  0.4057, -0.1516,  ...,  0.3333,  0.3333,  0.3333],\n",
       "        [-0.3825,  0.4057, -0.1516,  ...,  0.3333,  0.3333,  0.3333],\n",
       "        [-0.2397,  2.0000,  0.0769,  ...,  0.3333,  0.3333,  0.3333],\n",
       "        ...,\n",
       "        [ 2.0000,  0.4875, -0.0789,  ...,  0.3333,  0.3333,  0.3333],\n",
       "        [ 2.0000,  0.4168, -0.2405,  ...,  0.3333,  0.3333,  0.3333],\n",
       "        [-0.3825,  0.4307, -0.1016,  ...,  0.3333,  0.3333,  0.3333]],\n",
       "       device='cuda:0', grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_seq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e07060ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6095, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = (preds[mask]-seq[mask])**2\n",
    "out.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aefbafcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.2760,  5.9953,  3.8571,  5.9305,  6.1298,  1.8734,  4.6777,  5.6820,\n",
       "         1.9164,  1.5182,  6.0262,  4.8444,  8.8233,  5.1869,  6.2024,  2.4236,\n",
       "         1.9974,  6.4584,  3.1318,  4.0985,  1.6622, 12.9924,  2.1515,  4.6691,\n",
       "         0.8951,  6.0129,  2.5412,  5.8102,  5.3217,  3.2777,  4.2109,  5.6686,\n",
       "         4.4057,  3.8594,  3.0983,  1.4901,  1.6957,  5.5950,  2.5687, 11.3325,\n",
       "         2.8783,  2.3831,  4.2814,  4.3297,  5.3064,  2.6955,  3.4869,  3.6654,\n",
       "         3.3722,  5.5973,  4.7559,  4.7739,  0.5254,  3.5579,  3.1393,  3.6581,\n",
       "         1.6184,  7.2295,  4.4963,  3.7964,  5.5778,  3.2406,  7.6695,  5.4511,\n",
       "         5.0518,  2.3712,  2.1043,  0.5691,  2.8765,  2.4981,  4.3434,  7.6151,\n",
       "         5.1446,  9.0712,  2.5140,  5.4814,  6.6554,  3.2315,  7.5800,  5.2623,\n",
       "         7.6871,  6.2179,  2.9578,  1.8041,  7.0106,  2.2643,  5.5639,  7.4577,\n",
       "         1.3620,  6.4833,  2.6712, 10.9557,  1.7333,  2.0845,  7.9348,  9.6457,\n",
       "         1.3798,  2.2662,  7.1542,  8.3764,  4.5721,  3.2338,  4.7793,  1.9886,\n",
       "         2.0661,  5.8109,  4.6283,  3.5921, 14.7261,  5.1916,  6.5265,  5.7019,\n",
       "        10.0064,  4.7423,  4.6533,  5.6233,  2.1894,  2.8736,  3.3602,  2.3631,\n",
       "        13.9628,  2.2678,  3.2105,  3.1658,  2.5685,  5.3109,  1.9485, 11.4067,\n",
       "         3.5659,  4.7661,  2.7839,  1.2433,  5.2696,  3.3922], device='cuda:0',\n",
       "       grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds[mask]-seq[mask])**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76bbcee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0994, -0.1811,  2.0000, -0.0295,  0.3226,  0.6096,  1.0829,  0.5423,\n",
       "         2.0000, -0.0627,  2.0000,  0.7470,  0.9153,  0.0901,  0.5264,  0.5076,\n",
       "         0.3333,  0.3333,  0.3333,  0.3333,  0.3333,  0.3333,  0.3333,  0.3333,\n",
       "         0.3333,  0.3333,  0.3333,  0.3333,  0.3333,  0.3333], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_seq1[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aca8ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = mask * masked_replace_prob\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0e67b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0000, -0.3311,  0.1106,  ...,  0.3333,  0.3333,  0.3333],\n",
       "        [ 0.0994, -0.3311,  2.0000,  ...,  0.3333,  0.3333,  0.3333],\n",
       "        [ 0.2422, -0.1454,  2.0000,  ...,  0.3333,  0.3333,  0.3333],\n",
       "        ...,\n",
       "        [ 0.0994, -0.2493,  0.1833,  ...,  0.3333,  0.3333,  0.3333],\n",
       "        [ 0.0994, -0.3200,  0.0217,  ...,  0.3333,  0.3333,  0.3333],\n",
       "        [ 0.0994, -0.3061,  0.1606,  ...,  0.3333,  0.3333,  0.3333]],\n",
       "       device='cuda:0', grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_seq_pos.masked_fill(mask * masked_replace_prob, mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e312fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.3333, 0.2500, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.5000,\n",
       "        0.3333, 0.3750, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
       "        0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
       "        0.3333, 0.3333, 0.3333], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# derive labels to predict\n",
    "labels = seq # self.positional_encoding(seq) # add positional encoding before masking, so that encoding does not affect mask\n",
    "labels = labels.masked_fill(~mask, pad_token_id)\n",
    "\n",
    "labels[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6ef5e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels[0,1] == seq[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b6a06d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerWithCustomPositionalEncoding(\n",
      "  (positional_encoding): CustomPositionalEncoding()\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=30, out_features=30, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=30, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=30, bias=True)\n",
      "    (norm1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=30, out_features=30, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=30, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=30, bias=True)\n",
      "        (norm1): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((30,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Ok, let's see:\n",
    "trainer = TransformerWithCustomPositionalEncoding(num_heads = 1, \n",
    "                                                  num_layers= 1, \n",
    "                                                  embed_dim = num_fix).cuda()\n",
    "\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fb3021e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.2666, Test Loss: 0.0570\n",
      "Epoch 11: Train Loss: 0.0313, Test Loss: 0.0258\n",
      "Epoch 21: Train Loss: 0.0251, Test Loss: 0.0213\n",
      "Epoch 31: Train Loss: 0.0233, Test Loss: 0.0198\n",
      "Epoch 41: Train Loss: 0.0216, Test Loss: 0.0201\n"
     ]
    }
   ],
   "source": [
    "# # Setup optimizer to optimize model's parameters\n",
    "optimizer = Adam(trainer.parameters(), lr=3e-4)\n",
    "\n",
    "epochs = 41\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    ### Training\n",
    "    trainer.train()\n",
    "    \n",
    "    for X_train, y_train in train_loader:\n",
    "        # 1. Forward pass \n",
    "        # 2. Calculate loss/accuracy\n",
    "        train_preds, loss = trainer(X_train)\n",
    "        \n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backwards\n",
    "        loss.backward()\n",
    " \n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * X_train.size(0)\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    #### Evaluation\n",
    "    test_loss = 0.0\n",
    "    trainer.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            test_preds, tloss = trainer(X_test)\n",
    "            test_loss += tloss.item() * X_test.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "336f8c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.1111],\n",
       "        [0.2222],\n",
       "        [0.3333],\n",
       "        [0.3333],\n",
       "        [0.4444],\n",
       "        [0.5556],\n",
       "        [0.7778],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.8889],\n",
       "        [0.3333],\n",
       "        [0.3333],\n",
       "        [0.3333]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[1, 0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "724035a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0475, -0.2503, -0.4501, -0.4139, -0.2933,  0.3569,  0.4882,  0.6947,\n",
       "         0.4225,  0.8231,  0.7815,  0.5046,  0.3444,  0.3982, -0.0254],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds[1, 0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d243e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

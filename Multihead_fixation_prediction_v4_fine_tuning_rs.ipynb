{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "444f024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial, reduce\n",
    "import argparse\n",
    "import copy\n",
    "import time\n",
    "from datetime import datetime\n",
    "import gc\n",
    "\n",
    "from typing import TextIO, Callable, Collection, Dict, Iterator, List, Tuple, Type, TypeVar\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "T = TypeVar(\"T\", bound=\"EyetrackingClassifier\")\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"saved_models/transformer\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21cf40",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91aec7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "NUM_FEATURES = 14\n",
    "NUM_FIX = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8823925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def mask_with_tokens(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    return mask\n",
    "\n",
    "def mse_loss(target, input, mask):\n",
    "    out = (input[mask]-target[mask])**2\n",
    "    return out.mean()\n",
    "\n",
    "def mask_with_tokens_3D(t, token_ids):\n",
    "    init_no_mask = torch.full_like(t, False, dtype=torch.bool)\n",
    "    mask = reduce(lambda acc, el: acc | (t == el), token_ids, init_no_mask)\n",
    "    reduced = torch.any(mask, dim=-1, keepdim=True)\n",
    "    expanded = reduced.expand_as(mask)\n",
    "    return expanded\n",
    "\n",
    "def get_mask_subset_with_prob_3D(mask, prob):\n",
    "    batch, num_fix, num_features, device = *mask.shape, mask.device\n",
    "    max_masked = math.ceil(prob * num_fix)\n",
    "\n",
    "    num_tokens = mask.sum(dim=-2, keepdim=True)\n",
    "    mask_excess = (mask.cumsum(dim=-2)[:,:,0] > (num_tokens[:,:,0] * prob).ceil())\n",
    "    mask_excess = mask_excess[:, :max_masked]\n",
    "\n",
    "    rand = torch.rand((batch, num_fix, num_features), device=device).masked_fill(~mask, -1e9)\n",
    "    _, sampled_indices = rand.topk(max_masked, dim=-2)\n",
    "    sampled_indices = (sampled_indices[:,:,0] + 1).masked_fill_(mask_excess, 0)\n",
    "\n",
    "    new_mask = torch.zeros((batch, num_fix + 1), device=device)\n",
    "    new_mask.scatter_(-1, sampled_indices, 1)\n",
    "    new_mask = new_mask[:, 1:].bool()\n",
    "    \n",
    "    return new_mask.unsqueeze_(2).expand(-1,-1, num_features)\n",
    "    \n",
    "\n",
    "def prob_mask_like_3D(t, prob):\n",
    "    temp = torch.zeros_like(t[:,:,0]).float().uniform_(0, 1) < prob\n",
    "    return temp.unsqueeze_(2).expand(-1,-1, NUM_FEATURES)\n",
    "    \n",
    "    \n",
    "def pad_group_with_zeros(group, target_rows):\n",
    "    # Calculate the number of rows to add\n",
    "    num_missing_rows = target_rows - len(group)\n",
    "    if num_missing_rows > 0:\n",
    "        # Create a DataFrame with the required number of padding rows\n",
    "        # input padding\n",
    "        zero_rows = pd.DataFrame(0.3333, index=range(num_missing_rows), columns=group.columns)\n",
    "        # Label padding\n",
    "        # zero_rows.iloc[:, 0] = 31\n",
    "        # Concatenate the group with the zero rows\n",
    "        group = pd.concat([group, zero_rows], ignore_index=True)\n",
    "    return group\n",
    "\n",
    "# class ToTensor(object):\n",
    "#     \"\"\"Convert Series in sample to Tensors.\"\"\"\n",
    "\n",
    "#     def __call__(self, sample):\n",
    "#         trial, label = sample['trial'], sample['label']\n",
    "#         trial = torch.from_numpy(trial).float()\n",
    "#         label = torch.from_numpy(label).float()\n",
    "#         return trial, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5178796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPositionalEncoding(nn.Module):\n",
    "    \"\"\"Learnable positional encoding for both features and fixations\n",
    "    Assumes input `x` is of shape [batch_size, fixations, embed_dim]\"\"\"\n",
    "    def __init__(self, fixations = 30, features = None):\n",
    "        super(CustomPositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Initialize a learnable positional encoding matrix\n",
    "        self.encoding = nn.Parameter(torch.zeros(fixations, features)).to(device)\n",
    "        nn.init.xavier_uniform_(self.encoding)  # Xavier initialization for better training stability\n",
    "        \n",
    "    def forward(self, x, mask = None):\n",
    "        if mask is not None:\n",
    "            # Apply the mask to ignore padded positions\n",
    "            pos_encoding = self.encoding  * mask\n",
    "        else:\n",
    "            pos_encoding = self.encoding\n",
    "        return x + pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de096ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnasPositionalEncoding(nn.Module):\n",
    "    \"\"\"Learnable positional encoding for both features and fixations\n",
    "    Assumes input `x` is of shape [batch_size, fixations, embed_dim]\"\"\"\n",
    "    def __init__(self, fixations = int, features = int):\n",
    "        super(AnnasPositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Initialize a learnable positional encoding matrix for fixations\n",
    "        self.fix_encoding = nn.Parameter(torch.zeros(fixations, 1)).to(device)\n",
    "        nn.init.xavier_uniform_(self.fix_encoding)  # Xavier initialization for better training stability\n",
    "        self.fix_encoding = self.fix_encoding.expand(-1, features)\n",
    "        \n",
    "        # Initialize a learnable positional encoding matrix for features\n",
    "        self.feat_encoding = nn.Parameter(torch.zeros(1, features)).to(device)\n",
    "        nn.init.xavier_uniform_(self.feat_encoding)  # Xavier initialization for better training stability\n",
    "        self.feat_encoding = self.feat_encoding.expand(fixations, -1)\n",
    "        \n",
    "        self.encoding = self.fix_encoding + self.feat_encoding\n",
    "        \n",
    "    def forward(self, x, mask = None):\n",
    "        if mask is not None:\n",
    "            # Apply the mask to ignore padded positions\n",
    "            pos_encoding = self.encoding  * mask\n",
    "        else:\n",
    "            pos_encoding = self.encoding\n",
    "        return x + pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de3b7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsolutePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=278, l2norm_embed = False):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5 if not l2norm_embed else 1.\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.l2norm_embed = l2norm_embed\n",
    "        self.emb = nn.Embedding(max_seq_len, dim)\n",
    "\n",
    "    def forward(self, x, pos = None, seq_start_pos = None, mask = None):\n",
    "        seq_len, device = x.shape[1], x.device\n",
    "        assert seq_len <= self.max_seq_len, f'you are passing in a sequence length of {seq_len} but your absolute positional embedding has a max sequence length of {self.max_seq_len}'\n",
    "\n",
    "        #if not exists(pos):\n",
    "        pos = torch.arange(seq_len, device = device)\n",
    "\n",
    "        #if exists(seq_start_pos):\n",
    "        #    pos = (pos - seq_start_pos[..., None]).clamp(min = 0)\n",
    "\n",
    "        pos_emb = self.emb(pos)\n",
    "        pos_emb = pos_emb * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            # Apply the mask to ignore padded positions\n",
    "            pos_emb = pos_emb * mask\n",
    "            \n",
    "        return l2norm(pos_emb) if self.l2norm_embed else pos_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1ad59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=278):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model%2 != 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:,0:-1]\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).to(device)\n",
    "\n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        pos_enc = self.pe[:, :x.size(1)]\n",
    "        if not(mask == None):\n",
    "            mask = mask.to(device)\n",
    "            pos_enc = pos_enc*mask\n",
    "        return x + pos_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e4065",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f7ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinguisticFeature = Callable[[Tuple[str]], Tuple[torch.Tensor]]\n",
    "\n",
    "# def apply_standardization(x, m, sd):\n",
    "#     nonzero_sd = sd.clone()\n",
    "#     nonzero_sd[sd == 0] = 1\n",
    "#     x = torch.from_numpy(x).float()\n",
    "#     res = (x - m.unsqueeze(0)) / nonzero_sd.unsqueeze(0)\n",
    "#     return res\n",
    "\n",
    "def apply_standardization(x, m, sd):\n",
    "    nonzero_sd = sd.clone()\n",
    "    nonzero_sd[sd == 0] = 1\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x_zeros = x[x.sum(dim=(1)) == 0]\n",
    "    x_zeros[x_zeros==0] = -5\n",
    "    x_non_zeros = x[x.sum(dim=(1)) != 0]\n",
    "    x_non_zeros = (x_non_zeros - m.unsqueeze(0)) / nonzero_sd.unsqueeze(0)\n",
    "    res = torch.cat((x_non_zeros, x_zeros), axis =0)\n",
    "    return res\n",
    "\n",
    "\n",
    "def aggregate_per_subject(subjs, y_preds, y_preds_class, y_trues):\n",
    "    y_preds = np.array(y_preds)\n",
    "    y_preds_class = np.array(y_preds_class)\n",
    "    y_trues = np.array(y_trues)\n",
    "    subjs = np.array(subjs).flatten()\n",
    "    y_preds_subj = []\n",
    "    y_preds_class_subj = []\n",
    "    y_trues_subj = []\n",
    "    subjs_subj = np.unique(subjs)\n",
    "    for subj in subjs_subj:\n",
    "        subj = subj.item()\n",
    "        y_pred_class_subj = y_preds_class[subjs == subj]\n",
    "        y_pred_subj = y_preds[subjs == subj]\n",
    "        y_true_subj = y_trues[subjs == subj]\n",
    "        assert len(np.unique(y_true_subj)) == 1, f\"No unique label: subj={subj}\"\n",
    "        y_trues_subj.append(np.unique(y_true_subj).item())\n",
    "        y_preds_subj.append(np.mean(y_pred_subj).item())\n",
    "        if sum(y_pred_class_subj) >= (len(y_pred_class_subj) / 2):\n",
    "            y_preds_class_subj.append(1)\n",
    "        else:\n",
    "            y_preds_class_subj.append(0)\n",
    "    return subjs_subj, y_preds_subj, y_preds_class_subj, y_trues_subj\n",
    "\n",
    "def getmeansd(dataset, batch: bool = False):  # removing rows of 0s\n",
    "    if batch:\n",
    "        # Anna added preprocessing from ndarray to torch\n",
    "        tensors = [X for X, _, _, _ in dataset]  #torch.from_numpy(X).float()\n",
    "        tensors = torch.cat(tensors, axis=0)\n",
    "        # remove padded tensors\n",
    "        tensors = tensors[tensors.sum(dim=(1,2)) != 0]   #tensors[tensors.sum(dim=(1, 2)) != 0]\n",
    "        # remove rows of 0s from the computation\n",
    "        sentences, timesteps, features = tensors.size()\n",
    "        subset = tensors.sum(dim=(2)) != 0\n",
    "        subset = subset.view(sentences, timesteps, 1)\n",
    "        subset = subset.expand(sentences, timesteps, features)\n",
    "        result = tensors[subset].view(-1, features) \n",
    "        \n",
    "        means = torch.mean(result, dim=(0))\n",
    "        sd = torch.std(result, dim=(0))\n",
    "        return means, sd\n",
    "    else:\n",
    "        tensors = [torch.from_numpy(X).float() for X, _, _, _ in dataset] # Anna added , was [X for X, _, _ in dataset]\n",
    "        tensors = torch.cat(tensors, axis=0)\n",
    "        # remove padded tensors\n",
    "        tensors = tensors[tensors.sum(dim=1) != 0]\n",
    "        means = torch.mean(tensors, 0)\n",
    "        sd = torch.std(tensors, 0)\n",
    "        return means, sd\n",
    "    \n",
    "    \n",
    "# def get_params(paramdict) -> dict:\n",
    "#     selected_pars = dict()\n",
    "#     for k in paramdict:\n",
    "#         selected_pars[k] = random.sample(list(paramdict[k]), 1)[0]\n",
    "#     return selected_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b81deb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_speed_per_subject(subjs, y_preds, y_trues):\n",
    "    y_preds = np.array(y_preds)\n",
    "    #y_preds_class = np.array(y_preds_class)\n",
    "    y_trues = np.array(y_trues)\n",
    "    subjs = np.array(subjs).flatten()\n",
    "    y_preds_subj = []\n",
    "    y_trues_subj = []\n",
    "    subjs_subj = np.unique(subjs)\n",
    "    for subj in subjs_subj:\n",
    "        subj = subj.item()\n",
    "        y_pred_subj = y_preds[subjs == subj]\n",
    "        y_true_subj = y_trues[subjs == subj]\n",
    "        assert len(np.unique(y_true_subj)) == 1, f\"No unique label: subj={subj}\"\n",
    "        y_trues_subj.append(np.unique(y_true_subj).item())\n",
    "        y_preds_subj.append(np.mean(y_pred_subj).item())\n",
    "\n",
    "    return subjs_subj, y_preds_subj, y_trues_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e747c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyetrackingDataPreprocessor(Dataset):\n",
    "    \"\"\"Dataset with the long-format sequence of fixations made during reading by dyslexic \n",
    "    and normally-developing Russian-speaking monolingual children.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        csv_file, \n",
    "        transform=None, \n",
    "        target_transform=None,  \n",
    "        num_folds: float = 10,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            target_transform (callable, optional): Optional transform to be applied\n",
    "                on a label.\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # changing dyslexia labels to 0 and 1\n",
    "        if {'group'}.issubset(data.columns):   # not the case for pretrain dataset\n",
    "            data['group'] = data['group'] + 0.5\n",
    "        \n",
    "        # log-transforming frequency\n",
    "        to_transform = ['frequency', 'predictability', 'fix_dur'] #\n",
    "        for column in to_transform:\n",
    "            data[column] = data[column].apply(lambda x: np.log(x) if x > 0 else 0) \n",
    "        \n",
    "        # drop columns we don't use\n",
    "        data = data.drop(columns = ['fix_x', 'fix_y', 'fix_index'])  \n",
    "        \n",
    "        # center reading sopeed in case we need to predict it\n",
    "        if {'Reading_speed'}.issubset(data.columns):\n",
    "            data['Reading_speed'] = (data['Reading_speed'] - data['Reading_speed'].mean())/data['Reading_speed'].std(ddof=0)\n",
    "        \n",
    "        if {'sex', 'Grade'}.issubset(data.columns):\n",
    "            data = data.drop(columns = ['sex', 'Grade'])\n",
    "            \n",
    "        convert_columns = ['direction']\n",
    "        \n",
    "        if {'IQ', 'Sound_detection', 'Sound_change'}.issubset(data.columns):\n",
    "            data = data.drop(columns = ['IQ', 'Sound_detection', 'Sound_change'])\n",
    "        \n",
    "        for column in convert_columns:\n",
    "            prefix = column + '_dummy'\n",
    "            data = pd.concat([data, pd.get_dummies(data[column], \n",
    "                                    prefix=prefix)], axis=1)\n",
    "            data = data.drop(columns = column)\n",
    "\n",
    "        data.dropna(axis = 0, how = 'any', inplace = True)\n",
    "\n",
    "            \n",
    "        # rearrange columns (I need demogrpahic information to come last)\n",
    "#         cols = ['item', 'subj', 'group', 'Reading_speed', 'fix_dur', 'landing', 'word_length',\n",
    "#                  'predictability', 'frequency', 'number.morphemes', 'next_fix_dist',\n",
    "#                  'sac_ampl', 'sac_angle', 'sac_vel', 'rel.position', 'direction_dummy_DOWN',\n",
    "#                  'direction_dummy_LEFT', 'direction_dummy_RIGHT', 'direction_dummy_UP',\n",
    "#                  'sex', 'Age', 'Grade_dummy_1', 'Grade_dummy_2', 'Grade_dummy_3', 'Grade_dummy_4',\n",
    "#                  'Grade_dummy_5', 'Grade_dummy_6']\n",
    "        if {'Reading_speed'}.issubset(data.columns):\n",
    "            cols = ['item', 'subj', 'group', 'Reading_speed', 'fix_dur',\n",
    "                   'landing', 'word_length', 'predictability', 'frequency', \n",
    "                    'number.morphemes', 'next_fix_dist', 'sac_ampl', 'sac_angle', \n",
    "                    'sac_vel', 'rel.position', 'direction_dummy_LEFT', \n",
    "                    'direction_dummy_RIGHT', 'direction_dummy_DOWN'] # temporary\n",
    "        else:\n",
    "            cols = ['item', 'subj', 'fix_dur',\n",
    "                   'landing', 'word_length', 'predictability', 'frequency', \n",
    "                    'number.morphemes', 'next_fix_dist', 'sac_ampl', 'sac_angle', \n",
    "                    'sac_vel', 'rel.position', 'direction_dummy_LEFT', \n",
    "                    'direction_dummy_RIGHT', 'direction_dummy_DOWN'] # temporary\n",
    "        data = data[cols]\n",
    "        \n",
    "        # Record features that are used for prediction\n",
    "        if {'Reading_speed'}.issubset(data.columns):\n",
    "            self._features = [i for i in data.columns if i not in ['group', 'item', 'subj', 'Reading_speed']]\n",
    "        else:\n",
    "            self._features = [i for i in data.columns if i not in ['item', 'subj']]\n",
    "        self._data = pd.DataFrame()\n",
    "        # Add sentence IDs and subject IDs\n",
    "        self._data[\"sn\"] = data[\"item\"]\n",
    "        self._data[\"subj\"] = data[\"subj\"]\n",
    "        # Add labels\n",
    "        if {'Reading_speed'}.issubset(data.columns):\n",
    "            self._data[\"group\"] = data[\"group\"]\n",
    "            self._data[\"reading_speed\"] = data[\"Reading_speed\"]\n",
    "        else:\n",
    "            self._data[\"group\"] = -1\n",
    "            self._data[\"reading_speed\"] = -1\n",
    "        \n",
    "        # Add features used for prediction\n",
    "        for feature in self._features:\n",
    "            self._data[feature] = data[feature]\n",
    "\n",
    "#       # Distribute subjects across stratified folds\n",
    "        self._num_folds = num_folds\n",
    "        self._folds = [[] for _ in range(num_folds)]\n",
    "        just_subjects = self._data[\"subj\"].unique()\n",
    "        random.shuffle(just_subjects)\n",
    "        for i, subj in enumerate(just_subjects):\n",
    "            self._folds[i % num_folds].append(subj)\n",
    "#         dyslexic_subjects = self._data[self._data[\"group\"] == 1][\"subj\"].unique()\n",
    "#         control_subjects = self._data[self._data[\"group\"] == 0][\"subj\"].unique()\n",
    "#         random.shuffle(dyslexic_subjects)\n",
    "#         random.shuffle(control_subjects)\n",
    "#         for i, subj in enumerate(dyslexic_subjects):\n",
    "#             self._folds[i % num_folds].append(subj)\n",
    "#         for i, subj in enumerate(control_subjects):\n",
    "#             self._folds[num_folds - 1 - i % num_folds].append(subj)\n",
    "        for fold in self._folds:\n",
    "            random.shuffle(fold)\n",
    "\n",
    "    def _iter_trials(self, folds: Collection[int]) -> Iterator[pd.DataFrame]:\n",
    "        # Iterate over all folds\n",
    "        for fold in folds:\n",
    "            # Iterate over all subjects in the fold\n",
    "            for subj in self._folds[fold]:       # Anna: subj in fold?\n",
    "                subj_data = self._data[self._data[\"subj\"] == subj]\n",
    "                # Iterate over all sentences this subject read\n",
    "                for sn in subj_data[\"sn\"].unique():\n",
    "                    trial_data = subj_data[subj_data[\"sn\"] == sn]\n",
    "                    yield trial_data\n",
    "                    \n",
    "                    \n",
    "    def iter_folds(\n",
    "        self, folds: Collection[int]) -> Iterator[Tuple[torch.Tensor, torch.Tensor, int]]:\n",
    "        for trial_data in self._iter_trials(folds):\n",
    "            predictors = trial_data[self._features].to_numpy()\n",
    "            #predictors = np.reshape(predictors, (int(len(predictors)/278), 278, predictors.shape[1]))\n",
    "            label = trial_data[\"group\"].unique().item()\n",
    "            subj = trial_data[\"subj\"].unique().item()\n",
    "            reading_speed = trial_data[\"reading_speed\"].unique().item()\n",
    "            #  X = (time_steps, features)\n",
    "            X = predictors\n",
    "            y = torch.tensor(label, dtype=torch.float)\n",
    "            rs = torch.tensor(reading_speed , dtype=torch.float)\n",
    "            yield X, y, subj, rs\n",
    "                    \n",
    "\n",
    "    @property\n",
    "    def num_features(self) -> int:\n",
    "        \"\"\"Number of features per word (excluding word vector dimensions).\"\"\"\n",
    "        return len(self._features)\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def max_number_of_sentences(self):\n",
    "        data_copy = self._data.copy()\n",
    "        max_s_count = data_copy.groupby(by=\"subj\").sn.unique()\n",
    "        return max([len(x) for x in max_s_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c918bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyetrackingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocessor: EyetrackingDataPreprocessor,\n",
    "       # word_vector_model: WordVectorModel,\n",
    "        folds: Collection[int],\n",
    "        batch_subjects: bool = False,\n",
    "    ):\n",
    "        self.sentences = list(preprocessor.iter_folds(folds))\n",
    "        self._subjects = list(np.unique([subj for _, _, subj, _ in self.sentences]))\n",
    "        self.num_features = preprocessor.num_features# + word_vector_model.dimensions()\n",
    "        self.batch_subjects = batch_subjects\n",
    "        #self.max_sentence_length = preprocessor.max_sentence_length\n",
    "        self.max_number_of_sentences = preprocessor.max_number_of_sentences\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        if self.batch_subjects:\n",
    "            subject = self._subjects[index]\n",
    "            subject_sentences = [\n",
    "                (X, y, subj, rs) for X, y, subj, rs in self.sentences if subj == subject\n",
    "            ]\n",
    "            X = torch.stack([torch.FloatTensor(X) for X, _, _, _ in subject_sentences]) #[X for X, _, _ in subject_sentences] #torch.FloatTensor([X for X, _, _ in subject_sentences])\n",
    "            y = torch.stack([y for _, y, _, _ in subject_sentences]).unique().squeeze() \n",
    "            rs = torch.stack([rs for _, _, _, rs in subject_sentences]).unique().squeeze()\n",
    "            return X, y, subject, rs\n",
    "\n",
    "        else:\n",
    "            X, y, subj, rs = self.sentences[index] \n",
    "            return X, y, subj, rs\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.batch_subjects:\n",
    "            return len(self._subjects)\n",
    "        else:\n",
    "            return len(self.sentences)\n",
    "\n",
    "    def standardize(self, mean: torch.Tensor, sd: torch.Tensor):\n",
    "        self.sentences = [\n",
    "            (apply_standardization(X, mean, sd), y, subj, rs)\n",
    "            for X, y, subj, rs in self.sentences\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aec3c4",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f129fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):  \n",
    "    def __init__(self,\n",
    "                dim_upscale = int,\n",
    "                inner_dim_upscale = int,\n",
    "                num_heads = int, \n",
    "                num_layers = int, \n",
    "                dropout = 0\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_upscale = dim_upscale\n",
    "        self.inner_dim_upscale = inner_dim_upscale\n",
    "        \n",
    "        # layer norm for multi-head attention\n",
    "        self.attn_layer_norm = nn.LayerNorm(self.dim_upscale)\n",
    "        # layer norm for feedforward network\n",
    "        self.ffn_layer_norm = nn.LayerNorm(self.dim_upscale)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim = self.dim_upscale,  \n",
    "                                                 num_heads = self.num_heads, \n",
    "                                                 bias = True,\n",
    "                                                 batch_first = True)\n",
    "        # feed forward\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(self.dim_upscale, self.inner_dim_upscale, bias = True),\n",
    "            nn.LayerNorm(self.inner_dim_upscale),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(self.inner_dim_upscale, self.dim_upscale, bias = True)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor):\n",
    "        # pass embeddings through multi-head attention\n",
    "        x, attn_probs = self.attention(src, src, src, src_mask)\n",
    "\n",
    "        # residual add and norm\n",
    "        first_out = self.attn_layer_norm(x + src)\n",
    "\n",
    "        # position-wise feed-forward network\n",
    "        x2 = self.ff(first_out)\n",
    "\n",
    "        # residual add and norm\n",
    "        second_out = self.ffn_layer_norm(x2 + first_out)\n",
    "\n",
    "        return second_out, attn_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4390ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                dim_upscale = int,\n",
    "                inner_dim_upscale = int,\n",
    "                num_heads = int, \n",
    "                num_layers = int, \n",
    "                dropout = 0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.dim_upscale = dim_upscale\n",
    "        self.inner_dim_upscale = inner_dim_upscale\n",
    "        \n",
    "        # create n_layers encoders \n",
    "        self.layers = nn.ModuleList([EncoderLayer(\n",
    "                                    dim_upscale = self.dim_upscale,\n",
    "                                    num_heads = self.num_heads, \n",
    "                                    inner_dim_upscale = self.inner_dim_upscale,\n",
    "                                    dropout = self.dropout)\n",
    "                                     for layer in range(self.num_layers)])\n",
    "\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor):\n",
    "\n",
    "        # pass the sequences through each encoder\n",
    "        for layer in self.layers:\n",
    "            src, attn_probs = layer(src, src_mask)\n",
    "\n",
    "        self.attn_probs = attn_probs\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eb7e8ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EyetrackingClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int, config):\n",
    "        super().__init__()\n",
    "        self.initialize_model(input_size, config)\n",
    "        self.config = config\n",
    "\n",
    "    def initialize_model(self, input_size: int, config):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _predict(self, X: torch.Tensor, identity: bool = False, subj_mean: bool = False, pretrain: bool = False) -> torch.Tensor:\n",
    "        if pretrain:\n",
    "            y_preds, labels, mask = self(X, identity, pretrain=True)\n",
    "            return y_preds, labels, mask\n",
    "        else:\n",
    "            y_preds, labels, mask = self(X, identity, pretrain)\n",
    "            return y_preds, labels, mask\n",
    "        \n",
    "    @classmethod\n",
    "    def train_model(\n",
    "        cls: Type[T],\n",
    "        data: EyetrackingDataset,\n",
    "        min_epochs: int = 15,\n",
    "        max_epochs: int = 300,\n",
    "        dev_data: EyetrackingDataset = None,\n",
    "        device: str = \"cuda\",\n",
    "        config = None,\n",
    "        patience = 20,\n",
    "        pretrained_model: T = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[T, int]:\n",
    "        model = pretrained_model or cls(data.num_features, config, **kwargs) \n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        optimizer = Adam(model.parameters(), lr=config[\"lr\"])\n",
    "        epoch_count = 0\n",
    "        best_losses = [float(\"inf\")] * patience\n",
    "        for epoch in range(max_epochs):\n",
    "            # reshuffle data in each epoch\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                data,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True,\n",
    "            )\n",
    "            epoch_count += 1\n",
    "            epoch_loss = 0\n",
    "            for X, _, _, _ in loader:\n",
    "                X = X.to(device)\n",
    "                train_preds, labels, mask = model._predict(X, identity = False) \n",
    "                loss = mse_loss(\n",
    "                    labels,\n",
    "                    train_preds,\n",
    "                    mask\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            #avg_loss = epoch_loss/math.ceil(len(loader.dataset)/BATCH_SIZE)  # TODO: delete later\n",
    "            #print(f\"Epoch {epoch} done. Loss: {avg_loss}\")\n",
    "            \n",
    "            if dev_data is not None:\n",
    "                dev_loss = model.evaluate(dev_data, metric=\"loss\", device=device)\n",
    "                model.train()\n",
    "                # print(f\"Dev loss: {dev_accuracy} in Epoch {epoch}\")\n",
    "                if epoch > min_epochs and all(dev_loss > i for i in best_losses):\n",
    "                    epoch_count -= patience - best_losses.index(min(best_losses))\n",
    "                    break\n",
    "                else:\n",
    "                    best_losses.pop(0)\n",
    "                    best_losses.append(dev_loss)\n",
    "        print(f\"Model is donce. memory_allocated GB: {torch.cuda.memory_allocated(0)/1024/1024/1024}\")\n",
    "        free, total = torch.cuda.mem_get_info(device)\n",
    "        mem_used_MB = (total - free) / 1024 ** 2\n",
    "        print(\"Used memory (MB):\", mem_used_MB)\n",
    "        return model\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        data: EyetrackingDataset,\n",
    "        metric: str = \"loss\",\n",
    "        device: str = \"cuda\",\n",
    "    ) -> Tuple[float, float, float, float]:\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        loader = torch.utils.data.DataLoader(data, batch_size=BATCH_SIZE)\n",
    "        loss = 0\n",
    "        for X, _, _, _ in loader:\n",
    "            X = X.to(device)\n",
    "            dev_preds, labels, mask = self._predict(X, identity = False)\n",
    "            dloss = mse_loss(\n",
    "                labels,\n",
    "                dev_preds,\n",
    "                mask\n",
    "            )\n",
    "            loss += dloss.item() \n",
    "        avg_loss = loss /math.ceil(len(loader.dataset)/BATCH_SIZE) \n",
    "        if metric == \"loss\":\n",
    "            return avg_loss\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric '{metric}'\")\n",
    "\n",
    "class TransformerClassifier(EyetrackingClassifier):\n",
    "    def initialize_model(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        config,\n",
    "        embed_dim = NUM_FIX,\n",
    "        d_model = NUM_FEATURES,\n",
    "        dim_upscale = 128,\n",
    "        inner_dim_upscale = 4*128,\n",
    "        num_heads = 1, \n",
    "        num_layers = 1, \n",
    "        dropout = 0.15,\n",
    "        mask_prob = 0.2,\n",
    "        pad_token_id = -5,\n",
    "        mask_ignore_token_ids = []\n",
    "        ):\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.d_model = d_model\n",
    "        self.mask_prob = config[\"mask_prob\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.dropout = config[\"dropout\"]\n",
    "        self.dim_upscale = config[\"upscale_dim\"]\n",
    "        self.inner_dim_upscale = config[\"inner_dim\"]\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, self.pad_token_id])\n",
    "        \n",
    "        self.positional_encoding = AnnasPositionalEncoding(fixations = self.embed_dim, \n",
    "                                                           features = self.d_model)\n",
    "        \n",
    "        self.upscale = nn.Linear(self.d_model, self.dim_upscale, bias = True)\n",
    "        self.downscale = nn.Linear(self.dim_upscale, self.d_model, bias = True)\n",
    "        \n",
    "        self.encoder = Encoder(dim_upscale = self.dim_upscale, \n",
    "                               num_heads = self.num_heads, \n",
    "                               num_layers = self.num_layers,\n",
    "                               inner_dim_upscale = self.inner_dim_upscale, \n",
    "                               dropout = self.dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, identity = False, pretrain: bool = False):\n",
    "        \n",
    "        # do not mask [pad] tokens, or any other tokens in the tokens designated to be excluded ([cls], [sep])\n",
    "        # also do not include these special tokens in the tokens chosen at random\n",
    "        no_mask = mask_with_tokens_3D(input, self.mask_ignore_token_ids) \n",
    "        mask = get_mask_subset_with_prob_3D(~no_mask, self.mask_prob)\n",
    "        hidden = no_mask + mask # all elements that the model will not attend to\n",
    "\n",
    "        masked_seq = input.clone().detach().to(device)\n",
    "        \n",
    "        #  positional encoding\n",
    "        masked_seq_pos = self.positional_encoding(masked_seq, mask = None) #for hidden fixations ~ no_mask\n",
    "\n",
    "        # derive labels to predict\n",
    "        labels = input.masked_fill(~mask, self.pad_token_id)\n",
    "    \n",
    "        if identity:\n",
    "            attn_mask = no_mask[:,:,0]\n",
    "        else:\n",
    "            attn_mask = hidden[:,:,0]\n",
    "        \n",
    "        # Upscaling\n",
    "        masked_seq_upscaled = self.upscale(masked_seq_pos)\n",
    "        \n",
    "        # Encoder\n",
    "        out = self.encoder(masked_seq_upscaled, \n",
    "                           attn_mask)  \n",
    "        out = self.downscale(out)\n",
    "\n",
    "        return out, labels, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fca2ca8",
   "metadata": {},
   "source": [
    "## Loading pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ea68aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerClassifier(\n",
      "  (positional_encoding): AnnasPositionalEncoding()\n",
      "  (upscale): Linear(in_features=14, out_features=256, bias=True)\n",
      "  (downscale): Linear(in_features=256, out_features=14, bias=True)\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Dropout(p=0.3571428571428571, inplace=False)\n",
      "          (4): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model = TheModelClass(*args, **kwargs)\n",
    "# model.load_state_dict(torch.load( CHECKPOINT_PATH+\"/final_state_dict.pth\", weights_only=True))\n",
    "old_model = torch.load(CHECKPOINT_PATH+\"/final_full_model.pth\", weights_only=False)\n",
    "old_model.eval()\n",
    "\n",
    "# which layer to modify\n",
    "print(old_model)\n",
    "\n",
    "for param in old_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eda85b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.classifier = nn.Sequential(*list(model.children())[-1])\n",
    "# print(model.classifier)\n",
    "# torch.all(model.classifier[1].weight == model.upscale.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f218db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyetrackingClassifierRS(nn.Module):\n",
    "    def __init__(self, input_size: int, config, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.initialize_model(input_size, config, pretrained_model)\n",
    "        self.config = config\n",
    "\n",
    "    def initialize_model(self, input_size: int, config, pretrained_model):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _predict(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        y_preds = self(X)\n",
    "        return y_preds\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def train_model(\n",
    "        cls: Type[T],\n",
    "        data: EyetrackingDataset,\n",
    "        min_epochs: int = 15,\n",
    "        max_epochs: int = 200,\n",
    "        dev_data: EyetrackingDataset = None,\n",
    "        device: str = \"cuda\",\n",
    "        config = None,\n",
    "        patience = 10,\n",
    "        pretrained_model = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[T, int]:\n",
    "        model = cls(input_size = data.num_features, \n",
    "                    config = config, \n",
    "                    pretrained_model = pretrained_model, **kwargs) # this is initialization. Hallelujah!\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        optimizer = Adam(model.parameters(), lr=config[\"lr\"])\n",
    "        epoch_count = 0\n",
    "        best_losses = [float(\"inf\")] * patience\n",
    "        for epoch in range(max_epochs):\n",
    "            # reshuffle data in each epoch\n",
    "            loader = torch.utils.data.DataLoader(\n",
    "                data,\n",
    "                batch_size=config[\"batch_size\"],\n",
    "                shuffle=True,\n",
    "                # drop_last=True\n",
    "            )\n",
    "            epoch_count += 1\n",
    "            epoch_loss = 0\n",
    "            for X, _, _, rs in loader:\n",
    "                X = X.to(device)\n",
    "                rs = rs.to(device)\n",
    "                optimizer.zero_grad()\n",
    "              \n",
    "                rs_pred = model._predict(X).squeeze() \n",
    "                loss = loss_fn(rs.squeeze(), rs_pred)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "            # print(f\"Epoch {epoch} done. Loss: {epoch_loss}\")\n",
    "            if dev_data is not None:\n",
    "                dev_accuracy = model.evaluate(dev_data, metric=\"loss\", \n",
    "                                                  device=device, \n",
    "                                                  batch_size = config[\"batch_size\"])\n",
    "                model.train()\n",
    "                # print(f\"Dev loss: {dev_accuracy} in Epoch {epoch}\")\n",
    "                if epoch > min_epochs and all(dev_accuracy > i for i in best_losses):\n",
    "                    epoch_count -= patience - best_losses.index(min(best_losses))\n",
    "                    break\n",
    "                else:\n",
    "                    best_losses.pop(0)\n",
    "                    best_losses.append(dev_accuracy)\n",
    "        return model\n",
    "    \n",
    "    def predict_rs(\n",
    "        self,\n",
    "        data: EyetrackingDataset,\n",
    "        device: str = \"cuda\",\n",
    "        config = None,\n",
    "        per_subj: bool = True,\n",
    "    ):\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        loader = torch.utils.data.DataLoader(data, batch_size=config[\"batch_size\"])\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        subjs = []\n",
    "        for X, _, subj, rs in loader:\n",
    "            X = X.to(device)\n",
    "            rs = rs.to(device)\n",
    "            rs_pred = self._predict(X).squeeze()\n",
    "\n",
    "            y_preds.extend([i.item() for i in rs_pred])\n",
    "            y_trues.extend([i.item() for i in rs])\n",
    "            subjs.extend([i for i in subj])\n",
    "        if per_subj:\n",
    "            subjs, y_preds, y_trues = aggregate_speed_per_subject(\n",
    "                subjs, y_preds, y_trues\n",
    "            )\n",
    "        return y_preds, y_trues, subjs\n",
    "            \n",
    "    def evaluate(\n",
    "        self,\n",
    "        data: EyetrackingDataset,\n",
    "        print_report: bool = False,\n",
    "        metric: str = \"loss\",\n",
    "        device: str = \"cuda\",\n",
    "        batch_size = None,\n",
    "        per_subj: bool = True,\n",
    "    ) -> Tuple[float, float, float, float]:\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "        y_preds = []\n",
    "        y_trues = []\n",
    "        subjs = []\n",
    "        loss = 0\n",
    "        for X, _, subj, rs in loader:\n",
    "            X = X.to(device)\n",
    "            rs = rs.to(device)\n",
    "            rs_pred = self._predict(X).squeeze()\n",
    "\n",
    "            loss += loss_fn(rs_pred, rs.squeeze()).item() \n",
    "\n",
    "            y_preds.extend([i.item() for i in rs_pred])\n",
    "            y_trues.extend([i.item() for i in rs])\n",
    "            subjs.extend([i for i in subj])\n",
    "        avg_loss = loss /math.ceil(len(loader.dataset)/batch_size) \n",
    "        if per_subj:\n",
    "            subjs, y_preds, y_trues = aggregate_speed_per_subject(\n",
    "                subjs, y_preds, y_trues\n",
    "            )\n",
    "        if metric == \"loss\":\n",
    "            return avg_loss\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric '{metric}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "01a77197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I need to imitate the forward pass\n",
    "\n",
    "class BinaryTransformerClassifier(EyetrackingClassifierRS):\n",
    "    def initialize_model(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        config,\n",
    "        pretrained_model,\n",
    "        dim_upscale = 256,\n",
    "        mask_prob = 0,\n",
    "        pad_token_id = -5,\n",
    "        mask_ignore_token_ids = []\n",
    "        ):\n",
    "        \n",
    "        self.dim_upscale = dim_upscale\n",
    "\n",
    "        # token ids\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_ignore_token_ids = set([*mask_ignore_token_ids, self.pad_token_id])\n",
    "        \n",
    "        # pre-trained layers\n",
    "        self.positional_encoding = pretrained_model.positional_encoding\n",
    "        self.upscale = pretrained_model.upscale\n",
    "        self.encoder = pretrained_model.encoder\n",
    "        \n",
    "        # new trainable layers\n",
    "        self.tuning = nn.Sequential(\n",
    "            nn.Linear(self.dim_upscale, 20, bias = True),\n",
    "            nn.Linear(20, 10 ,bias = True),\n",
    "            nn.Linear(10, 1 ,bias = True)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        no_mask = mask_with_tokens_3D(input, self.mask_ignore_token_ids) \n",
    "        masked_seq = input.clone().detach().to(device)\n",
    "        \n",
    "        #  positional encoding\n",
    "        masked_seq_pos = self.positional_encoding(masked_seq, mask = None) #for hidden fixations ~ no_mask\n",
    "    \n",
    "        attn_mask = no_mask[:,:,0]\n",
    "        upscale_mask = ~attn_mask.unsqueeze(2).expand(-1,-1, self.dim_upscale)\n",
    "        # Upscaling\n",
    "        masked_seq_upscaled = self.upscale(masked_seq_pos)\n",
    "        \n",
    "        # Encoder\n",
    "        out = self.encoder(masked_seq_upscaled, \n",
    "                           attn_mask) \n",
    "        \n",
    "        # Trainable level\n",
    "        mean_out = (out*upscale_mask).sum(dim=1)/upscale_mask.sum(dim=1) # Result: torch.Size([256]), upscale number of features\n",
    "#        first_out = out[:, 0, :]  \n",
    "#        last_out = out[:, 29, :]  \n",
    "\n",
    "        # Select the last existing fixation in the sentence (and not the padding)\n",
    "#         batch, fix, features = out.shape\n",
    "#         last_element = upscale_mask\n",
    "#         for i in range(last_element.shape[0]):  \n",
    "#             for k in range(last_element.shape[2]):  \n",
    "#                 true_indices = torch.where(last_element[i, :, k])[0]  \n",
    "#                 if len(true_indices) > 0:\n",
    "#                     last_element[i, true_indices[:-1], k] = False  # Keep only the last True\n",
    "#         last_out = out[last_element].reshape(batch, features)\n",
    "        pred = self.tuning(mean_out)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b233a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_space = {\n",
    "    \"transformer_tuning\": {\n",
    "        \"batch_size\": [128],           #8, 16, 32, 64, 128\n",
    "        \"lr\": [1e-02, 1e-03, 1e-04, 1e-5],     #np.linspace(1e-5, 1e-1, num=15),    #loguniform.rvs(1e-5, 1e-1, size=15)1e-02, 1e-03,\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_params(paramdict) -> dict:\n",
    "    selected_pars = dict()\n",
    "    for k in paramdict:\n",
    "        selected_pars[k] = random.sample(list(paramdict[k]), 1)[0]\n",
    "    return selected_pars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e89795",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3674131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test fold  0\n",
      "dev fold 1\n",
      "tune set 0\n",
      "0.4490472743908564\n",
      "0.45504522720972695\n",
      "dev fold 2\n",
      "tune set 0\n",
      "0.3976493775844574\n",
      "0.39502600487321615\n",
      "dev fold 3\n",
      "tune set 0\n",
      "0.4151140461365382\n",
      "0.41053440173467\n",
      "best performing parameter for dev fold  3 :  {'batch_size': 128, 'lr': 0.0001, 'decision_boundary': 0.5}\n",
      "test accuraccy fold  0\n",
      "test fold  1\n",
      "dev fold 0\n",
      "tune set 0\n",
      "0.3844295144081116\n",
      "0.39383011870086193\n",
      "dev fold 2\n",
      "tune set 0\n",
      "0.3879513777792454\n",
      "0.39306747168302536\n",
      "dev fold 3\n",
      "tune set 0\n",
      "0.3937177509069443\n",
      "0.41318339308102925\n",
      "best performing parameter for dev fold  3 :  {'batch_size': 128, 'lr': 0.0001, 'decision_boundary': 0.5}\n",
      "test accuraccy fold  1\n",
      "test fold  2\n",
      "dev fold 0\n",
      "tune set 0\n",
      "0.3882975000888109\n",
      "0.39890895038843155\n",
      "dev fold 1\n",
      "tune set 0\n",
      "0.4551410923401515\n",
      "0.45165120661258695\n",
      "dev fold 3\n",
      "tune set 0\n",
      "0.39904244045416515\n",
      "0.4077179193496704\n",
      "best performing parameter for dev fold  3 :  {'batch_size': 128, 'lr': 0.0001, 'decision_boundary': 0.5}\n",
      "test accuraccy fold  2\n",
      "test fold  3\n",
      "dev fold 0\n",
      "tune set 0\n",
      "0.4021823266521096\n",
      "0.4017913555726409\n",
      "dev fold 1\n",
      "tune set 0\n",
      "0.460398139556249\n",
      "0.4558829794327418\n",
      "dev fold 2\n",
      "tune set 0\n",
      "0.38889026176184416\n",
      "0.3839020626619458\n",
      "best performing parameter for dev fold  3 :  {'batch_size': 128, 'lr': 0.0001, 'decision_boundary': 0.5}\n",
      "test accuraccy fold  3\n",
      "used test params:  [{'batch_size': 128, 'lr': 0.0001, 'decision_boundary': 0.5}, {'batch_size': 128, 'lr': 0.0001, 'decision_boundary': 0.5}, {'batch_size': 128, 'lr': 0.0001, 'decision_boundary': 0.5}, {'batch_size': 128, 'lr': 0.0001, 'decision_boundary': 0.5}]\n",
      "mean: 0.40381949649502835 std: 0.014790364719673677\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# default: --no-tune --wordvectors none --model cnn --subjpred\n",
    "parser = argparse.ArgumentParser(description=\"Run Russian Eye-Movement Pretraining\")\n",
    "parser.add_argument(\"--model\", dest=\"model\")\n",
    "parser.add_argument(\"--tunesets\", type=int, default=2)\n",
    "parser.add_argument(\"--tune\", dest=\"tune\", action=\"store_true\")\n",
    "parser.add_argument(\"--no-tune\", dest=\"tune\", action=\"store_false\")\n",
    "parser.add_argument(\"--pretrain\", dest=\"pretrain\", action=\"store_true\")\n",
    "parser.add_argument(\"--seed\", dest=\"seed\", type=int, default=42)\n",
    "parser.add_argument(\"--cuda\", dest=\"cudaid\", default=0)\n",
    "parser.set_defaults(tune=True) \n",
    "parser.set_defaults(model = \"transformer_tuning\")\n",
    "args = parser.parse_args(args=[]) # modified to work with jupyter notebook\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "if args.model == \"transformer_tuning\":\n",
    "    MODEL_CLASS = BinaryTransformerClassifier \n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    device = torch.device(f'cuda:{args.cudaid}')\n",
    "    \n",
    "\n",
    "    \n",
    "NUM_FOLDS = 4\n",
    "NUM_TUNE_SETS = args.tunesets\n",
    "tune = args.tune\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "BATCH_SUBJECTS = False\n",
    "\n",
    "file = 'data/30fixations_no_padding_sentence_word_pos.csv'  # data with dyslexia labels\n",
    "\n",
    "used_test_params = []\n",
    "parameter_sample = [\n",
    "    get_params(hyperparameter_space[\"transformer_tuning\"]) for _ in range(NUM_TUNE_SETS)\n",
    "]\n",
    "\n",
    "\n",
    "tprs_folds = {}\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# load and preprocess data for training\n",
    "preprocessor = EyetrackingDataPreprocessor(\n",
    "    csv_file = file, \n",
    "   num_folds = NUM_FOLDS\n",
    ")\n",
    "\n",
    "test_accuracies = []\n",
    "\n",
    "for test_fold in range(NUM_FOLDS):\n",
    "    print(\"test fold \", test_fold)\n",
    "    parameter_evaluations = np.zeros(shape=(NUM_FOLDS, NUM_TUNE_SETS))\n",
    "    if tune:\n",
    "        # Normal training / fine-tuning\n",
    "        for dev_fold in range(NUM_FOLDS):\n",
    "            if args.pretrain:\n",
    "                pretrained_models = next(pretrained_model_generator)\n",
    "            if dev_fold == test_fold:\n",
    "                continue\n",
    "            print(f'dev fold {dev_fold}')\n",
    "            train_folds = [\n",
    "                fold\n",
    "                for fold in range(NUM_FOLDS)\n",
    "                if fold != test_fold and fold != dev_fold\n",
    "            ]\n",
    "\n",
    "            train_dataset = EyetrackingDataset(\n",
    "                preprocessor,\n",
    "                folds=train_folds,\n",
    "                batch_subjects=BATCH_SUBJECTS,\n",
    "            )\n",
    "            mean, sd = getmeansd(train_dataset, batch=BATCH_SUBJECTS)\n",
    "            train_dataset.standardize(mean, sd)\n",
    "            dev_dataset = EyetrackingDataset(\n",
    "                preprocessor,\n",
    "                folds=[dev_fold],\n",
    "                batch_subjects=BATCH_SUBJECTS,\n",
    "            )\n",
    "            dev_dataset.standardize(mean, sd)\n",
    "            for tune_set in range(NUM_TUNE_SETS):\n",
    "                running_model = copy.deepcopy(MODEL_CLASS)\n",
    "                if tune_set%100 == 0:\n",
    "                    print(f'tune set {tune_set}')\n",
    "                model = None\n",
    "                model = running_model.train_model(\n",
    "                            train_dataset,\n",
    "                            min_epochs=15,\n",
    "                            max_epochs=300,\n",
    "                            dev_data=dev_dataset,\n",
    "                            pretrained_model=old_model,\n",
    "                            device=device,\n",
    "                            config= parameter_sample[tune_set]\n",
    "                        )\n",
    "                tune_accuracy = model.evaluate(\n",
    "                    data=dev_dataset,\n",
    "                    device=device,\n",
    "                    metric=\"loss\",\n",
    "                    batch_size = parameter_sample[tune_set][\"batch_size\"]\n",
    "                )\n",
    "                parameter_evaluations[dev_fold, tune_set] = tune_accuracy\n",
    "                print(tune_accuracy)\n",
    "                del running_model\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            # Select best parameter set\n",
    "        mean_dev_loss = np.mean(parameter_evaluations, axis=0)\n",
    "        best_parameter_set = np.argmin(mean_dev_loss)\n",
    "        params_test = parameter_sample[best_parameter_set]\n",
    "        print(f'best performing parameter for dev fold ', dev_fold, \": \", params_test)\n",
    "        used_test_params.append(params_test)\n",
    "        best_pretrained_model = None\n",
    "        \n",
    "    running_model = copy.deepcopy(MODEL_CLASS)\n",
    "    dev_fold = (test_fold + 1) % NUM_FOLDS\n",
    "    train_folds = [\n",
    "        fold for fold in range(NUM_FOLDS) if fold != test_fold and fold != dev_fold\n",
    "    ]\n",
    "    train_dataset = EyetrackingDataset(\n",
    "        preprocessor,\n",
    "        folds=train_folds,\n",
    "        batch_subjects=BATCH_SUBJECTS,\n",
    "    )\n",
    "    mean, sd = getmeansd(train_dataset, batch=BATCH_SUBJECTS)\n",
    "    train_dataset.standardize(mean, sd)\n",
    "    dev_dataset = EyetrackingDataset(\n",
    "        preprocessor,\n",
    "        folds=[dev_fold],\n",
    "        batch_subjects=BATCH_SUBJECTS\n",
    "    )\n",
    "    dev_dataset.standardize(mean, sd)\n",
    "    test_dataset = EyetrackingDataset(\n",
    "        preprocessor,\n",
    "        folds=[test_fold],\n",
    "        batch_subjects=BATCH_SUBJECTS,\n",
    "    )\n",
    "    test_dataset.standardize(mean, sd)\n",
    "    model = running_model.train_model(\n",
    "        train_dataset,\n",
    "        min_epochs=15,\n",
    "        max_epochs=200,\n",
    "        dev_data=dev_dataset,\n",
    "        pretrained_model=old_model,\n",
    "        device=device,\n",
    "        config=params_test,\n",
    "    )\n",
    "    print(f'test accuraccy fold ', test_fold)\n",
    "    test_accuracy = model.evaluate(\n",
    "        test_dataset,\n",
    "        device=device,\n",
    "        metric=\"loss\",\n",
    "        print_report=True,\n",
    "        per_subj=BATCH_SUBJECTS,\n",
    "        batch_size = params_test[\"batch_size\"]\n",
    "    )\n",
    "    # print(\"test acc fold \", test_fold, \" : \", test_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "if tune:\n",
    "    print(\"used test params: \", used_test_params)\n",
    "print(\n",
    "    \"mean:\",\n",
    "    np.mean(test_accuracies, axis=0),\n",
    "    \"std:\",\n",
    "    np.std(test_accuracies, axis=0) / np.sqrt(NUM_FOLDS),\n",
    ")\n",
    "\n",
    "pred_level = \"subjectpred\" if BATCH_SUBJECTS else \"textpred\"\n",
    "final_scores_mean = np.mean(test_accuracies, axis=0)\n",
    "final_scores_std = np.std(test_accuracies, axis=0) / np.sqrt(NUM_FOLDS)\n",
    "\n",
    "# out_str = \"\"\n",
    "# with open(f\"reader_prediction_results/reading_speed_info/{args.model}_scores_{pred_level}.txt\", \"w\") as f:\n",
    "#     for i in range(len(final_scores_mean)):\n",
    "#         out_str += f\"${round(final_scores_mean[i],2):1.2f}\\db{{{round(final_scores_std[i],2):1.2f}}}$\"\n",
    "#         if i < len(final_scores_mean) - 1:\n",
    "#             out_str += \" & \"\n",
    "#         else:\n",
    "#             out_str += \" \\\\\\\\ \"\n",
    "#     f.write(out_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
